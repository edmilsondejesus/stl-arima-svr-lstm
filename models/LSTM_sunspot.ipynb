{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install pandas\n",
        "#!pip install seaborn\n",
        "#!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "naOfrff2U2Xy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow import data\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fix random seed for reproducibility\n",
        "tf.random.set_seed(9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5iIpJyzkmkW",
        "outputId": "00380b11-657f-4c91-cbc3-1194bcb79e40"
      },
      "outputs": [],
      "source": [
        "# #from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "file_data = 'SN_m_tot_V2.0.csv'\n",
        "path_name='../datasets/'\n",
        "path_name_results='../results/'\n",
        "file_result = 'Result_LSTM_sunspot.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "JPemdWEukqf-"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(f'{path_name}{file_data}', sep =';', encoding = 'latin1', decimal='.',usecols=[3])\n",
        "dataset.columns = ['num_observations']\n",
        "\n",
        "#dataset = pd.read_csv(f'{path_name}{file_data}', sep =';', encoding = 'latin1', decimal='.')\n",
        "#dataset.columns = ['year','month', 'date','total_sunspot_number','std_derivation','num_observations','def_prov_indicator']\n",
        "#dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[104.3],\n",
              "       [116.7],\n",
              "       [ 92.8],\n",
              "       ...,\n",
              "       [163.4],\n",
              "       [159.1],\n",
              "       [114.9]])"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3295 entries, 0 to 3294\n",
            "Data columns (total 1 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   num_observations  3295 non-null   float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 25.9 KB\n"
          ]
        }
      ],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "num_observations    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checks if there are null variables\n",
        "dataset.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "def salvar_resultado(nm_dataset, ds_best_param, n_time_steps, MSE, RMSE, MAE, MAPE, sMAPE, Duration):\n",
        "  #Script to write training cycle results\n",
        "  data = [nm_dataset, ds_best_param, n_time_steps, MSE, RMSE, MAE, MAPE, sMAPE, Duration]\n",
        "  fields = ['Dataset','Best Params','n_time_steps','MSE', 'RMSE', 'MAE', 'MAPE','sMAPE','Duration']\n",
        "  with open(f'{path_name_results}{file_result}', \"a\",newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file,delimiter=';')\n",
        "    writer.writerow(data)  \n",
        "  print(fields)\n",
        "  print(data)\n",
        "    \n",
        "#Script to create the results file\n",
        "def criar_arquivo_resultado():\n",
        "  fields = ['Dataset','Best Params','n_time_steps','MSE', 'RMSE', 'MAE','MAPE','sMAPE','Duration']\n",
        "  with open(f'{path_name_results}{file_result}', \"w\",newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file,delimiter=';')\n",
        "    writer.writerow(fields)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_matrix_dataset(dataset, n_time_steps=1):\n",
        " dX, dY = [], []\n",
        " for i in range(len(dataset)-n_time_steps-1):\n",
        "  a = dataset[i:(i+n_time_steps), 0]\n",
        "  dX.append(a)\n",
        "  dY.append(dataset[i + n_time_steps, 0])\n",
        " return np.array(dX), np.array(dY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "  \n",
        "def save_model(model,n_time_steps):\n",
        "  # serialize model to JSON\n",
        "  model_json = model.to_json()\n",
        "  with open(f'{path_name_results}model_{n_time_steps}.json', \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(f'{path_name_results}model_{n_time_steps}.h5')\n",
        "  print(\"Saved model to disk\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gera_resultado(y_test, predict,nm_dataset, resultado, n_time_steps, Duracao):\n",
        " #Mean Squared Error (Mean Squared Difference Between Estimated Values and Actual Values) - MSE\n",
        " MSE = mean_squared_error(y_test, predict)    \n",
        " #Square Root of Mean Error - RMSE\n",
        " RMSE = np.sqrt(mean_squared_error(y_test, predict))    \n",
        " #Mean Absolute Distance or Mean Absolute Error - MAE\n",
        " MAE= median_absolute_error(y_pred=predict, y_true = y_test) \n",
        "  \n",
        " #Calculate the MAPE (Mean Absolute Percentage Error)\n",
        " MAPE = ((np.mean(np.abs(y_test -predict) / (y_test)))) * 100   \n",
        "  \n",
        " sMAPE = round(\n",
        " \tnp.mean(\n",
        " \t\tnp.abs(predict - y_test) /\n",
        " \t\t((np.abs(predict) + np.abs(y_test)))\n",
        " \t)*100, 2\n",
        " ) \n",
        " salvar_resultado(nm_dataset, resultado, n_time_steps, MSE, RMSE, MAE, MAPE, sMAPE, Duracao)\n",
        " return MAPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "def previsao_LSTM(nm_dataset, dataset, n_time_steps, l1, l2, l3,num_epochs, batch_size): \n",
        " #Split dataset in treinam /  80% treinam  20% test\n",
        " dataset=np.array(dataset) \n",
        " nlinhas = int(len(dataset) * 0.80)\n",
        " test = dataset[nlinhas:len(dataset),:]  \n",
        " train = dataset[0:nlinhas,:] \n",
        " #  reshape into X=t and Y=t+1 ot n_time_steps by steps\n",
        " #n_time_steps = 5\n",
        " X_train, Y_train = create_matrix_dataset(train, n_time_steps)\n",
        " X_test, Y_test = create_matrix_dataset(test, n_time_steps) \n",
        " #X_train.shape , Y_train.shape , X_test.shape , Y_test.shape  \n",
        " #reshape input to be [samples, time steps, features]\n",
        " X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        " X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) \n",
        " # Stores the training execution start time\n",
        " Hora_Inicio = time.time()\n",
        "   \n",
        " resultado = \"LSTM (\" + str(l1) + \",\" + str(l2) + \",\" + str(l3) + \") n_time_steps=\" + str(n_time_steps) + str(\" epochs=\") + str(num_epochs)\n",
        " print(resultado)\n",
        "\n",
        " # create and fit the LSTM network\n",
        " steps_per_epoch = len(X_train) \n",
        " model = Sequential()\n",
        " model.add(LSTM(l1, batch_input_shape=(batch_size, n_time_steps, 1 ), stateful=True, return_sequences=True))\n",
        " model.add(LSTM(l2, batch_input_shape=(batch_size, n_time_steps, 1 ), stateful=True, return_sequences=True))\n",
        " model.add(LSTM(l3, batch_input_shape=(batch_size, n_time_steps, 1 ), stateful=True))\n",
        " model.add(Dense(1))\n",
        " model.compile(loss='mean_squared_error', optimizer='adam')\n",
        " #model.add(Dense(1, activation='relu'))\n",
        " \n",
        " #model.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mape'])     \n",
        " \n",
        " \n",
        " #equalize train data to be multiple of batch_size\n",
        " train_size = int(np.trunc(len(X_train) / batch_size))\n",
        " X_train = X_train[0:(train_size * batch_size),:]\n",
        " Y_train = Y_train[0:(train_size * batch_size)]\n",
        " \n",
        " #equalize test data to be multiple of batch_size\n",
        " test_size = int(np.trunc(len(X_test) / batch_size))\n",
        " X_test = X_test[0:(test_size * batch_size),:]\n",
        " Y_test = Y_test[0:(test_size * batch_size)]\n",
        " \n",
        " for i in range(num_epochs):\t\n",
        "   model.fit(X_train, Y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
        "   model.reset_states()\n",
        " \n",
        " #predict values to X_test with size multiple of batch_size\n",
        " predict = model.predict(X_test, batch_size=batch_size)   \n",
        " \n",
        " Y_test = [Y_test]\n",
        " predict = predict.reshape(-1,1)[0:batch_size]   \n",
        " Y_test=np.array(Y_test[0][0:batch_size]).reshape(batch_size,1) \n",
        " \n",
        " #Y_test = [Y_test]\n",
        " #testPredict = predict.reshape(-1,1)   \n",
        " #Y_test=Y_test[0][0:batch_size]\n",
        " #predict=testPredict[0:batch_size]    \n",
        " #Y_test=np.array(Y_test).reshape(24,1) \n",
        " \n",
        " Hora_Fim = time.time()   \n",
        " #Calculate the duration of the training execution\n",
        " Duracao = Hora_Fim - Hora_Inicio   \n",
        " y_test = Y_test\n",
        " \n",
        " #calc metrics of error and save in file\n",
        " return gera_resultado(y_test, predict,nm_dataset, resultado, n_time_steps, Duracao)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forecast for new sunspot number\n",
            "LSTM (90,90,90) n_time_steps=1 epochs=100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 24s 24ms/step - loss: 9155.7412\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 7928.3906\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 7179.9570\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 6596.8882\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 6140.5659\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 5781.5303\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 5493.2725\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 5270.7837\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 5100.4580\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 4970.2334\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 4871.1152\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 4795.2646\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 3974.0264\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 3596.2178\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 3352.8640\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 3141.7913\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 2955.9526\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 2781.0979\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 2626.0051 0s - loss: 2692.\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 2486.4336\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 2358.4221\n",
            "109/109 [==============================] - ETA: 0s - loss: 2260.85 - 3s 30ms/step - loss: 2243.1572\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 2134.6775\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 2046.5583\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1956.4717\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 1880.5905\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 1807.1552\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 1737.0585\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 1673.7771\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 1607.6881\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 1554.0978\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 1499.3854\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1449.2321 \n",
            "109/109 [==============================] - 4s 32ms/step - loss: 1427.1709\n",
            "109/109 [==============================] - 4s 32ms/step - loss: 1366.3956\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 1330.7301\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 1295.0874\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 1271.4011\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 1226.3383\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 1196.8324\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 1156.9958\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1122.3490\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1102.8033\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 1092.4886\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 1053.0637\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 1030.4315\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 1008.5818\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 983.5540\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 979.2706\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 959.0529\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 949.3850\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 942.2814\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 925.9705\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 942.1871\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 906.6234\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 902.8973\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 874.3461\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 860.4265\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 842.2571\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 833.2314\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 822.9277\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 824.8454\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 806.8872\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 803.8860\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 798.3201\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 788.4166\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 779.3925\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 784.7546\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 782.7712\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 784.3063\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 784.1736\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 769.6318\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 759.5435\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 750.4551\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 744.3878\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 730.4282\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 720.6375\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 723.2584\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 726.0098\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 722.3643\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 700.2820\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 702.0773\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 685.0297\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 691.8482\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 686.1078\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 675.0486\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 679.2617\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 671.0246\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 666.5306\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 673.0924\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 682.9988\n",
            "109/109 [==============================] - 3s 32ms/step - loss: 688.8928\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 662.1088\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 661.0069\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 650.4944\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 652.2103\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 653.0256\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 650.1859\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 659.3573: 1s\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 653.8685\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=1 epochs=100', 1, 436.99043052452276, 20.90431607406764, 11.405146789550784, 9.982162485547093, 5.29, 364.58464074134827]\n",
            "LSTM (90,90,90) n_time_steps=2 epochs=100\n",
            "109/109 [==============================] - 20s 36ms/step - loss: 8975.7266\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 7778.5044\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 7055.3770\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 6495.5713\n",
            "109/109 [==============================] - 4s 32ms/step - loss: 5967.7856\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 5351.7026\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 4884.6675\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 4478.7188\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 4125.9097\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 3811.3640\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 3536.5806\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 3292.6060\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 3078.6775\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 2888.6738\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 2718.6143\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 2566.1433\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 2425.5974\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 2298.1184\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 2178.8704\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 2076.6035\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 1985.8439\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1901.9087\n",
            "109/109 [==============================] - 3s 32ms/step - loss: 1812.2557\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 1737.2859 0s - l\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 1667.0649\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 1600.3961\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 1531.0525 0s - loss: 1555.\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 1479.4984\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 1426.8678\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 1387.3595\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 1341.7642\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 1302.3864\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1266.8855\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1220.7043\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 1192.4602\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 1157.6167\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1132.4320\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1109.1971\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1091.5619\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 1067.4401\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 1041.1610\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1024.1210 0s \n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1003.0869\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 1003.1914\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 972.3436- ETA: 0s - loss: \n",
            "109/109 [==============================] - 2s 22ms/step - loss: 949.0582\n",
            "109/109 [==============================] - 4s 39ms/step - loss: 944.9678\n",
            "109/109 [==============================] - 8s 58ms/step - loss: 919.4467\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 899.5921\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 897.6017\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 875.4603\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 859.5826\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 847.5901\n",
            "109/109 [==============================] - 4s 39ms/step - loss: 835.3370\n",
            "109/109 [==============================] - 4s 40ms/step - loss: 825.6476\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 805.3370\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 802.0059\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 783.6975\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 781.0646\n",
            "109/109 [==============================] - 2s 19ms/step - loss: 762.1945\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 757.4722\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 755.7267\n",
            "109/109 [==============================] - 2s 20ms/step - loss: 751.5376\n",
            "109/109 [==============================] - 2s 18ms/step - loss: 733.5032\n",
            "109/109 [==============================] - 2s 19ms/step - loss: 742.5071\n",
            "109/109 [==============================] - 2s 19ms/step - loss: 742.9215\n",
            "109/109 [==============================] - 2s 20ms/step - loss: 727.6463\n",
            "109/109 [==============================] - 2s 17ms/step - loss: 724.7843\n",
            "109/109 [==============================] - 2s 18ms/step - loss: 708.7661\n",
            "109/109 [==============================] - 2s 17ms/step - loss: 707.1882\n",
            "109/109 [==============================] - 2s 16ms/step - loss: 701.9467\n",
            "109/109 [==============================] - 2s 19ms/step - loss: 694.7240\n",
            "109/109 [==============================] - 2s 17ms/step - loss: 684.8723\n",
            "109/109 [==============================] - 2s 19ms/step - loss: 686.4875\n",
            "109/109 [==============================] - 2s 16ms/step - loss: 681.7389\n",
            "109/109 [==============================] - 2s 20ms/step - loss: 681.1461\n",
            "109/109 [==============================] - 2s 17ms/step - loss: 671.7363\n",
            "109/109 [==============================] - 2s 19ms/step - loss: 674.9331\n",
            "109/109 [==============================] - 2s 17ms/step - loss: 673.9295\n",
            "109/109 [==============================] - 2s 18ms/step - loss: 665.8654\n",
            "109/109 [==============================] - 2s 16ms/step - loss: 658.5148\n",
            "109/109 [==============================] - 2s 16ms/step - loss: 656.8296\n",
            "109/109 [==============================] - 2s 19ms/step - loss: 655.9861\n",
            "109/109 [==============================] - 2s 16ms/step - loss: 657.5854\n",
            "109/109 [==============================] - 2s 18ms/step - loss: 656.4230\n",
            "109/109 [==============================] - 2s 17ms/step - loss: 640.4019\n",
            "109/109 [==============================] - 2s 18ms/step - loss: 633.6894\n",
            "109/109 [==============================] - 2s 18ms/step - loss: 634.9563\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 623.7904\n",
            "109/109 [==============================] - 2s 18ms/step - loss: 615.3373\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 651.8425\n",
            "109/109 [==============================] - 2s 20ms/step - loss: 617.2575\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 607.1937\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 626.6900\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 628.8243\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 600.2917\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 589.3214\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 584.1909\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 583.2717\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 594.9401\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=2 epochs=100', 2, 307.1652169754714, 17.52612954920371, 11.25103302001952, 9.188267756200135, 4.63, 338.9168629646301]\n",
            "LSTM (90,90,90) n_time_steps=3 epochs=100\n",
            "109/109 [==============================] - 11s 21ms/step - loss: 9063.5352\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 7893.2490\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 7158.9482\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 6583.6328\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 6112.4746\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 5468.5669\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 4978.7847\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 4563.4839\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 4199.5503\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 3878.3425\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 3592.6128\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 3341.1628\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 3119.9036\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 2926.0793\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 2750.7795\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 2595.0330\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 2452.5842\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 2324.1365\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 2210.4866\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 2104.2917\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 2008.9780\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 1919.6393\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 1840.1165\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 1763.0081\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1690.7659\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1635.1957\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 1570.7599\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 1501.6154\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1445.5490\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1388.6963\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1350.9612\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1301.5795\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 1251.1161\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1214.8505\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 1178.8137\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1148.1543\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1109.8662\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1088.8999\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 1053.1320\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 1028.0359\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 1001.4293\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 969.1824\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 943.8718\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 930.2164\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 904.5244\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 885.1610\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 887.5779\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 854.4046\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 839.2410\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 813.9363\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 799.0389\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 793.3565\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 868.0589\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 743.9224\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 734.2570\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 727.2388\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 718.4294\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 721.6504\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 703.1735\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 687.7729\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 673.0436\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 659.2308\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 646.0932\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 628.6865\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 633.3481\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 625.3525\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 623.3821\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 618.7697\n",
            "109/109 [==============================] - 3s 27ms/step - loss: 599.3167\n",
            "109/109 [==============================] - 2s 23ms/step - loss: 595.5349\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 587.3628\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 576.8731\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 561.3113\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 554.4125\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 540.2189\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 540.8215\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 562.9789\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 524.9310\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 510.3814\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 510.2596\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 539.5839\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 576.5557\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 525.1490\n",
            "109/109 [==============================] - 3s 23ms/step - loss: 514.1887\n",
            "109/109 [==============================] - 2s 22ms/step - loss: 502.9456\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 507.6351\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 492.7120\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 469.6378\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 487.4877\n",
            "109/109 [==============================] - 3s 24ms/step - loss: 469.4257\n",
            "109/109 [==============================] - 3s 25ms/step - loss: 473.2866\n",
            "109/109 [==============================] - 2s 21ms/step - loss: 445.8232\n",
            "109/109 [==============================] - 3s 26ms/step - loss: 452.3002\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 465.4577\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 524.8039\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 453.4080\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 422.8484\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 434.6466\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 427.0308\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 434.8015\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=3 epochs=100', 3, 393.6204826469891, 19.839871034031173, 11.01046600341796, 10.74766341002839, 5.43, 304.96568727493286]\n",
            "LSTM (90,90,90) n_time_steps=4 epochs=100\n",
            "109/109 [==============================] - 16s 36ms/step - loss: 8992.9326\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 7801.0552\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 7089.0825\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 6493.9165\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 5821.1670\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 5291.2017\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 4834.5557\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 4435.6592\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 4082.3481\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 3773.2126\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 3499.3704\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 3258.4304\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 3048.2495\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 2861.6079\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 2755.3076\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 2621.1514\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 2426.0046\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 2288.1887\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 2174.0078\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 2067.9089\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1970.2024\n",
            "109/109 [==============================] - 4s 39ms/step - loss: 1880.8364\n",
            "109/109 [==============================] - 4s 39ms/step - loss: 1798.2955\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1718.0541\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1645.5753\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1578.0696\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 1516.5697\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 1458.2582\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1404.9347\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1357.6393\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1315.0570\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1267.6705\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 1235.4456\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 1192.2239\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1159.6639\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 1127.7015\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 1090.3248\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 1056.0900\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 1020.1679\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 991.9661\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 965.2218\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 937.7610\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 908.3802\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 909.4146\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 880.3682\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 896.4377\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 873.6685\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 892.4158\n",
            "109/109 [==============================] - 3s 32ms/step - loss: 802.6132\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 787.7874\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 773.1440\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 750.0816\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 729.1655\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 720.5995\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 710.1823\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 698.0096\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 671.5923\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 667.6843\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 650.8310\n",
            "109/109 [==============================] - 3s 28ms/step - loss: 666.6216\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 638.4187\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 625.3495\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 620.7310\n",
            "109/109 [==============================] - 3s 31ms/step - loss: 598.4390\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 583.2475\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 560.3207\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 555.4014\n",
            "109/109 [==============================] - 3s 32ms/step - loss: 540.1222\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 534.5897\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 573.6000\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 514.9214\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 495.1873\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 475.0760\n",
            "109/109 [==============================] - 4s 35ms/step - loss: 480.2033\n",
            "109/109 [==============================] - 4s 33ms/step - loss: 469.4587\n",
            "109/109 [==============================] - 4s 32ms/step - loss: 449.2785\n",
            "109/109 [==============================] - 3s 32ms/step - loss: 441.4323\n",
            "109/109 [==============================] - 4s 32ms/step - loss: 466.9004\n",
            "109/109 [==============================] - 4s 34ms/step - loss: 447.8752\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 434.0631\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 423.7793\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 460.5677\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 437.1231\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 416.3087\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 389.5624\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 388.0229\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 399.0659\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 438.9336\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 434.3165\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 453.8076\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 419.6568\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 406.5237\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 396.3222\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 384.5568\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 404.7949\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 392.4028\n",
            "109/109 [==============================] - 3s 30ms/step - loss: 381.7289\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 367.2214\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 375.5283\n",
            "109/109 [==============================] - 3s 29ms/step - loss: 368.7955\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=4 epochs=100', 4, 339.81689053296276, 18.434122993323083, 11.163575744628915, 9.980024611936107, 4.97, 394.09912633895874]\n",
            "LSTM (90,90,90) n_time_steps=5 epochs=100\n",
            "109/109 [==============================] - 10s 30ms/step - loss: 9119.4277\n",
            "109/109 [==============================] - 4s 40ms/step - loss: 7932.5366\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 7187.5801\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 6500.1812\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 5867.6372\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 5343.5347\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 4885.5020\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 4485.7354\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 4128.6987\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 3817.2690\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 3540.0066\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 3293.7629\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 3081.3545\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 2890.7107\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 2724.2361\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 2567.2083\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 2429.8896\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 2301.1099\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 2188.5859\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 2082.6167\n",
            "109/109 [==============================] - 4s 39ms/step - loss: 1981.2930\n",
            "109/109 [==============================] - 5s 41ms/step - loss: 1893.5323\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1808.6160\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1725.0172\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1654.2053\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 1588.2965\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 1530.6635\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1471.6415\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1424.9998\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1366.6309\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1322.6471\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 1276.8369\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1238.7153\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1194.1311\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1165.9969\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 1129.2592\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 1101.2808\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1073.1584\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1045.6499\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 1011.4644\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 989.7779\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 954.2805\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 935.7937\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 905.1259\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 890.9657\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 859.9855\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 859.7446\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 832.7715\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 812.2688\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 784.4236\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 776.2419\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 748.8434\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 731.5683\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 721.4746\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 709.0107\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 686.9415\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 675.4310\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 686.7582\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 657.6597\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 638.3847\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 631.0032\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 609.9189\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 596.5217\n",
            "109/109 [==============================] - 4s 39ms/step - loss: 610.3599\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 584.1548\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 571.1825\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 565.6872\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 553.9244\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 554.7040\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 535.8461\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 530.5175\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 543.2751\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 521.0481\n",
            "109/109 [==============================] - 4s 39ms/step - loss: 541.0515\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 511.4118\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 495.3023\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 501.6711\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 475.9992\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 465.7038\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 459.2604\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 471.3197\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 470.8273\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 478.0148\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 457.6663\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 450.4833\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 463.9248\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 451.6460\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 425.8873\n",
            "109/109 [==============================] - 4s 39ms/step - loss: 422.7298\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 417.0262\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 402.0152\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 417.5699\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 394.2601\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 379.8808\n",
            "109/109 [==============================] - 4s 38ms/step - loss: 385.9366\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 406.4285\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 393.2821\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 366.8507\n",
            "109/109 [==============================] - 4s 37ms/step - loss: 336.4549\n",
            "109/109 [==============================] - 4s 36ms/step - loss: 324.9088\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=5 epochs=100', 5, 381.6431555482596, 19.53568927753151, 15.265326690673824, 11.154613447941175, 5.52, 426.7145662307739]\n",
            "LSTM (90,90,90) n_time_steps=6 epochs=100\n",
            "109/109 [==============================] - 10s 38ms/step - loss: 9055.9980\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 7877.2354\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 7144.0767\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 6433.8301\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 5824.6509\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 5305.1924\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 4851.1973\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 4451.4937\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 4103.1929\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 3793.5295\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 3519.6541\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 3281.1108\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 3066.6995\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 2878.0500\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 2709.1538\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 2558.2502\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 2425.5625\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 2292.6782\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 2182.9072\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 2076.2029\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 1982.4111\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1897.3303\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 1822.4146\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 1751.0824\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 1677.2334\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 1609.8977\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1539.7468\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1477.2915\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1422.4154\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 1373.0573\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1330.4360\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 1280.4901\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1229.9374\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1202.8989\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 1160.6925\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1128.1389\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 1084.7933\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 1061.3657\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 1030.8416\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1004.9341\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 979.3014\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 954.2870\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 941.2153\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 898.9539\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 904.0157\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 886.3627\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 855.6108\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 835.1022\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 812.8682\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 804.5419\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 801.8549\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 767.7924\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 748.9805\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 724.2554\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 709.3061\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 688.7877\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 680.7086\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 657.0627\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 645.9540\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 634.1494\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 632.5773\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 620.7253\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 598.6283\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 582.0791\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 564.5453\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 545.6606\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 530.8018\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 515.9529\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 515.4716\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 538.8213\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 507.2898\n",
            "109/109 [==============================] - 7s 62ms/step - loss: 499.1198\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 461.8748\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 463.3694\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 442.5716\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 437.1053\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 463.8833\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 484.2512\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 448.4427\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 456.2721\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 409.5074\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 421.2591\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 394.5143\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 369.0315\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 376.6825\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 362.8428\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 356.9062\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 358.3832\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 366.6658\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 361.5672\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 361.8000\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 350.9976\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 346.6459\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 345.4033\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 321.1438\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 309.1962\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 349.4716\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 345.0340\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 356.8956\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 318.2518\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=6 epochs=100', 6, 700.4390086570314, 26.465808294042926, 17.86632537841797, 15.308530906991571, 6.63, 505.2708730697632]\n",
            "LSTM (90,90,90) n_time_steps=7 epochs=100\n",
            "109/109 [==============================] - 11s 39ms/step - loss: 9220.8223\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 8019.9272\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 7263.6826\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 6651.5635\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 5982.1011\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 5435.7197\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 4966.0957\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 4555.0801\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 4195.9854\n",
            "109/109 [==============================] - 6s 51ms/step - loss: 3877.8772\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 3590.7661\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 3340.8049\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 3123.2593\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 2925.4563\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 2758.3843\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 2604.9666\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 2466.6477\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 2334.1404\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 2217.3242\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 2110.4832\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 2013.3737\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 1916.5774\n",
            "109/109 [==============================] - 5s 50ms/step - loss: 1834.8462\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 1758.6089\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 1683.8536\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1627.9362\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 1562.9525\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1501.1028\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1437.4413\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 1384.3827\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 1336.3499\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1291.0502\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1247.3392\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1204.3057\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1164.3442\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 1136.5829\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 1110.4081\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1077.4100\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1067.2080\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 1048.8308\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 1013.4226\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 970.2380\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 981.2880\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 919.0521\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 901.7203\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 875.0668\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 853.7686\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 850.4339\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 833.2536\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 807.0171\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 784.4119\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 790.3863\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 769.6492\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 744.6305\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 723.2455\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 716.6155\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 703.5431\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 692.6156\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 670.6238\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 655.0784\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 665.5634\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 659.7933\n",
            "109/109 [==============================] - 5s 50ms/step - loss: 630.0555\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 624.6341\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 610.9484\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 612.7719\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 605.8721\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 615.9789\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 610.2991\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 578.9704\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 576.8610\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 574.9133\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 569.4171\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 537.8047\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 527.6597\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 511.8921\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 508.9661\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 550.6604\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 524.3535\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 482.7448\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 482.8901\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 508.8541\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 456.4976\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 478.8953\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 454.7251\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 432.1628\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 437.8107\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 444.2914\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 428.0517\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 405.7499\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 398.0983\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 400.6279\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 409.4245\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 407.0627\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 393.8499\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 426.5427\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 376.9061\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 374.2509\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 336.1912\n",
            "109/109 [==============================] - 5s 47ms/step - loss: 358.9976\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=7 epochs=100', 7, 451.68426103652547, 21.252864772461276, 18.981419372558605, 14.0989554550782, 6.6, 539.1726613044739]\n",
            "LSTM (90,90,90) n_time_steps=8 epochs=100\n",
            "109/109 [==============================] - 11s 46ms/step - loss: 8967.2920\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 7796.5200\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 7074.9512\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 6372.0889\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 5769.8506\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 5251.6265\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 4803.1973\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 4408.6426\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 4065.4177\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 3757.9465\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 3489.1404\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 3249.0588\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 3034.0640\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 2852.7175\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 2687.6863\n",
            "109/109 [==============================] - 7s 61ms/step - loss: 2533.1521\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 2398.5774\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 2276.3557\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 2164.6445\n",
            "109/109 [==============================] - 6s 60ms/step - loss: 2068.5955\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 1969.6859\n",
            "109/109 [==============================] - 6s 59ms/step - loss: 1885.4264\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 1802.6256\n",
            "109/109 [==============================] - 6s 59ms/step - loss: 1717.6392\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 1644.0562\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 1581.3301\n",
            "109/109 [==============================] - 7s 60ms/step - loss: 1512.3427\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 1463.0236\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 1407.0414\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 1357.9117\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 1327.6730\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 1269.2286\n",
            "109/109 [==============================] - 7s 65ms/step - loss: 1242.4723\n",
            "109/109 [==============================] - 7s 65ms/step - loss: 1635.1997\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 1274.3882\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 1198.0887\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 1157.1283\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 1096.5599\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 1060.7697\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 1029.9092\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 994.1371\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 960.7885\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 926.1744\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 907.6281\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 883.6929\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 860.2964\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 833.2927\n",
            "109/109 [==============================] - 7s 65ms/step - loss: 822.7924\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 803.7897\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 807.0919\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 769.0233\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 746.9304\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 731.1560\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 700.4143\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 688.4366\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 676.3588\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 659.4076\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 657.1843\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 646.9719\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 638.4178\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 651.1579\n",
            "109/109 [==============================] - 7s 60ms/step - loss: 636.8622\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 613.5888\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 601.7930\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 601.5767\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 595.3961\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 589.9806\n",
            "109/109 [==============================] - 7s 62ms/step - loss: 569.3777\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 574.6709\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 540.2776\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 528.1997\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 514.2180\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 495.5653\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 488.7264\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 481.9120\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 455.1594\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 449.9614\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 443.2249\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 438.8214\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 419.2277\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 422.4193\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 440.3930\n",
            "109/109 [==============================] - 6s 59ms/step - loss: 435.4264\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 408.8890\n",
            "109/109 [==============================] - 7s 62ms/step - loss: 380.0962\n",
            "109/109 [==============================] - 7s 62ms/step - loss: 372.6447\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 393.4799\n",
            "109/109 [==============================] - 6s 60ms/step - loss: 377.4066\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 363.2565\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 361.9858\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 353.8047\n",
            "109/109 [==============================] - 6s 55ms/step - loss: 345.6651\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 342.3480\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 326.8395\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 329.6996\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 377.3983\n",
            "109/109 [==============================] - 6s 56ms/step - loss: 373.6462\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 343.1614\n",
            "109/109 [==============================] - 6s 58ms/step - loss: 328.2847\n",
            "109/109 [==============================] - 6s 57ms/step - loss: 311.4027\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=8 epochs=100', 8, 289.5285269452325, 17.015537809462046, 14.055587768554688, 11.188978009791532, 5.39, 641.697084903717]\n",
            "LSTM (90,90,90) n_time_steps=9 epochs=100\n",
            "109/109 [==============================] - 13s 63ms/step - loss: 9275.6084\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 8086.9106\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 7325.1797\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 6706.4004\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 6037.4429\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 5496.2939\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 5027.5801\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 4616.8242\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 4254.5010\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 3937.0566\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 3653.0093\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 3401.1650\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 3176.5823\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 2983.7827\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 2810.1218\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 2648.3428\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 2497.1562\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 2367.6069\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 2246.3596\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 2135.5569\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 2035.5533\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 1955.5724\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 1861.4712\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 1785.0671\n",
            "109/109 [==============================] - 7s 68ms/step - loss: 1697.4016\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 1616.8669\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 1561.2651\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 1567.6559\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 1520.8099\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 1456.9778\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 1454.5016\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 1420.4648\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 1348.8167\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 1316.7124\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 1231.8502\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 1158.0459\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 1116.0396\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 1094.1230\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 1055.9940\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 1007.2595\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 988.8128\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 955.9282\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 931.5687\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 922.0078\n",
            "109/109 [==============================] - 8s 77ms/step - loss: 895.9781\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 890.6849\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 854.2046\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 830.1748\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 812.4261\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 782.7085\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 764.8668\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 755.4099\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 724.5073\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 698.1427\n",
            "109/109 [==============================] - 10s 90ms/step - loss: 697.5595\n",
            "109/109 [==============================] - 8s 77ms/step - loss: 669.6202\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 639.8545\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 645.5172\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 640.5449\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 612.7554\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 587.2615\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 569.8956\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 591.2440\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 551.6868\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 543.5983\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 526.0281\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 530.0438\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 496.1398\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 480.7627\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 479.4388\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 451.8484\n",
            "109/109 [==============================] - 7s 69ms/step - loss: 441.8766\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 447.9413\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 426.4101\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 417.5214\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 434.2094\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 425.2192\n",
            "109/109 [==============================] - 8s 72ms/step - loss: 416.7021\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 386.5178\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 364.1701\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 346.9068\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 336.5465\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 349.5769\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 356.7519\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 350.7092\n",
            "109/109 [==============================] - 7s 65ms/step - loss: 362.2152\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 337.4706\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 342.4907\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 337.6613\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 296.1540\n",
            "109/109 [==============================] - 7s 69ms/step - loss: 293.6389\n",
            "109/109 [==============================] - 7s 69ms/step - loss: 286.5686\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 288.8633\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 270.6937\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 283.4261\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 366.0845\n",
            "109/109 [==============================] - 8s 69ms/step - loss: 303.9668\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 296.8893\n",
            "109/109 [==============================] - 8s 71ms/step - loss: 268.4991\n",
            "109/109 [==============================] - 8s 70ms/step - loss: 254.2179\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=9 epochs=100', 9, 460.39186294427935, 21.45674399680155, 13.75633697509766, 15.02633924747905, 7.01, 801.2323920726776]\n",
            "LSTM (90,90,90) n_time_steps=10 epochs=100\n",
            "109/109 [==============================] - 14s 66ms/step - loss: 9188.8262\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 8001.7080\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 7248.4531\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 6628.7173\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 5945.8115\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 5408.1357\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 4936.8403\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 4522.8965\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 4162.3296\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 3851.4277\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 3572.8586\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 3318.6858\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 3096.9248\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 2900.0754\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 2728.0056\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 2578.9160\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 2448.6685\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 2312.6726\n",
            "109/109 [==============================] - 8s 77ms/step - loss: 2193.9873\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 2088.5491\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1989.1946\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1900.1399\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 1820.6777\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1742.9004\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 1669.0966\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 1606.1007\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 1537.1171\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1478.2642\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1422.5402\n",
            "109/109 [==============================] - 8s 73ms/step - loss: 1368.5017\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1314.2157\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 1273.4670\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 1238.7177\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 1189.1005\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 1157.4988\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1123.2523\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 1086.0358\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1059.8384\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 1031.4666\n",
            "109/109 [==============================] - 8s 77ms/step - loss: 1005.0554\n",
            "109/109 [==============================] - 9s 79ms/step - loss: 974.2166\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 955.1242\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 931.5784\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 914.3633\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 880.4371\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 869.5544\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 876.9975\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 891.6763\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 851.4235\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 831.4662\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 810.9654\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 784.8550\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 765.4847\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 762.1415\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 759.5943\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 708.3978\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 701.7202\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 680.1418\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 667.7185\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 669.3319\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 666.1119\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 641.7571\n",
            "109/109 [==============================] - 8s 78ms/step - loss: 628.8254\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 619.4907\n",
            "109/109 [==============================] - 9s 79ms/step - loss: 609.4649\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 619.6750\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 594.3311\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 596.4720\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 588.4843\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 579.7686\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 572.5721\n",
            "109/109 [==============================] - 8s 78ms/step - loss: 561.3024\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 533.6155\n",
            "109/109 [==============================] - 8s 78ms/step - loss: 536.5065\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 535.5327\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 528.3300\n",
            "109/109 [==============================] - 8s 77ms/step - loss: 513.5489\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 524.7056\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 506.1302\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 518.9614\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 509.5951\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 489.9964\n",
            "109/109 [==============================] - 8s 78ms/step - loss: 484.3677\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 477.7708\n",
            "109/109 [==============================] - 8s 74ms/step - loss: 472.8698\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 462.7142\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 449.3951\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 489.5334\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 438.9856\n",
            "109/109 [==============================] - 8s 78ms/step - loss: 427.4504\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 413.9295\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 405.6613\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 413.2700\n",
            "109/109 [==============================] - 9s 79ms/step - loss: 427.4451\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 420.5301\n",
            "109/109 [==============================] - 8s 78ms/step - loss: 433.3003\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 411.5167\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 424.0778\n",
            "109/109 [==============================] - 8s 76ms/step - loss: 409.5276\n",
            "109/109 [==============================] - 8s 75ms/step - loss: 388.6114\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=10 epochs=100', 10, 434.1025289271624, 20.835127283680375, 13.649939727783199, 14.171990958166766, 6.69, 844.7791774272919]\n",
            "LSTM (90,90,90) n_time_steps=11 epochs=100\n",
            "109/109 [==============================] - 15s 81ms/step - loss: 9129.6689\n",
            "109/109 [==============================] - 9s 87ms/step - loss: 7924.3921\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 7185.1338\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 6602.1826\n",
            "109/109 [==============================] - 9s 87ms/step - loss: 5932.4092\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 5373.5962\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 4900.2944\n",
            "109/109 [==============================] - 11s 96ms/step - loss: 4491.5562\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 4131.8735\n",
            "109/109 [==============================] - 10s 91ms/step - loss: 3817.0608\n",
            "109/109 [==============================] - 14s 129ms/step - loss: 3540.7512\n",
            "109/109 [==============================] - 16s 147ms/step - loss: 3294.7161\n",
            "109/109 [==============================] - 19s 173ms/step - loss: 3080.0911\n",
            "109/109 [==============================] - 13s 121ms/step - loss: 2888.3025\n",
            "109/109 [==============================] - 10s 94ms/step - loss: 2716.6558\n",
            "109/109 [==============================] - 10s 95ms/step - loss: 2562.1370\n",
            "109/109 [==============================] - 10s 94ms/step - loss: 2423.6519\n",
            "109/109 [==============================] - 11s 100ms/step - loss: 2297.3525\n",
            "109/109 [==============================] - 11s 103ms/step - loss: 2178.6628\n",
            "109/109 [==============================] - 11s 104ms/step - loss: 2081.6013\n",
            "109/109 [==============================] - 11s 103ms/step - loss: 1981.6385\n",
            "109/109 [==============================] - 11s 101ms/step - loss: 1899.0332\n",
            "109/109 [==============================] - 11s 102ms/step - loss: 1823.4457\n",
            "109/109 [==============================] - 11s 99ms/step - loss: 1740.9073\n",
            "109/109 [==============================] - 11s 101ms/step - loss: 1666.1477\n",
            "109/109 [==============================] - 11s 102ms/step - loss: 1595.6429\n",
            "109/109 [==============================] - 11s 99ms/step - loss: 1532.7467\n",
            "109/109 [==============================] - 11s 100ms/step - loss: 1475.5177\n",
            "109/109 [==============================] - 11s 97ms/step - loss: 1422.6786\n",
            "109/109 [==============================] - 10s 96ms/step - loss: 1370.0526\n",
            "109/109 [==============================] - 10s 95ms/step - loss: 1321.5575\n",
            "109/109 [==============================] - 11s 99ms/step - loss: 1276.5931\n",
            "109/109 [==============================] - 11s 102ms/step - loss: 1243.9766\n",
            "109/109 [==============================] - 10s 96ms/step - loss: 1201.4325\n",
            "109/109 [==============================] - 13s 115ms/step - loss: 1163.3409\n",
            "109/109 [==============================] - 11s 104ms/step - loss: 1131.5249\n",
            "109/109 [==============================] - 11s 103ms/step - loss: 1095.4629\n",
            "109/109 [==============================] - 10s 95ms/step - loss: 1061.5844\n",
            "109/109 [==============================] - 11s 98ms/step - loss: 1041.3075\n",
            "109/109 [==============================] - 11s 98ms/step - loss: 1006.8185\n",
            "109/109 [==============================] - 10s 90ms/step - loss: 994.8171\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 972.7161\n",
            "109/109 [==============================] - 10s 87ms/step - loss: 954.8879\n",
            "109/109 [==============================] - 9s 87ms/step - loss: 963.5829\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 908.4099\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 906.8527\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 861.6328\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 838.4512\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 819.6533\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 801.7233\n",
            "109/109 [==============================] - 10s 87ms/step - loss: 788.3536\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 783.1230\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 760.3431\n",
            "109/109 [==============================] - 10s 90ms/step - loss: 763.3613\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 747.7203\n",
            "109/109 [==============================] - 10s 90ms/step - loss: 726.6228\n",
            "109/109 [==============================] - 9s 87ms/step - loss: 712.3240\n",
            "109/109 [==============================] - 11s 98ms/step - loss: 707.2983\n",
            "109/109 [==============================] - 10s 89ms/step - loss: 693.8250\n",
            "109/109 [==============================] - 10s 87ms/step - loss: 695.4802\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 668.7819\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 678.8307\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 683.8731\n",
            "109/109 [==============================] - 9s 87ms/step - loss: 657.0128\n",
            "109/109 [==============================] - 10s 92ms/step - loss: 640.1582\n",
            "109/109 [==============================] - 9s 87ms/step - loss: 636.6346\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 646.4849\n",
            "109/109 [==============================] - 10s 90ms/step - loss: 625.8542\n",
            "109/109 [==============================] - 10s 89ms/step - loss: 618.7127\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 604.7606\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 623.5716\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 595.1104\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 635.2615\n",
            "109/109 [==============================] - 9s 87ms/step - loss: 580.4796\n",
            "109/109 [==============================] - 10s 89ms/step - loss: 567.9648\n",
            "109/109 [==============================] - 10s 93ms/step - loss: 556.0149\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 550.7158\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 536.6445\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 556.7874\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 540.6621\n",
            "109/109 [==============================] - 9s 87ms/step - loss: 520.0789\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 516.6784\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 502.6787\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 492.4770\n",
            "109/109 [==============================] - 10s 89ms/step - loss: 494.9207\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 482.0141\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 476.6928\n",
            "109/109 [==============================] - 10s 89ms/step - loss: 470.4270\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 478.8470\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 478.6726\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 462.6703\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 460.2740\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 442.8581\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 431.9413\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 456.3973\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 481.6909\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 434.7011\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 427.8014\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 447.7095\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 445.1902\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=11 epochs=100', 11, 433.155787061135, 20.81239503423705, 14.405877685546884, 14.148994791495117, 6.57, 1035.6890966892242]\n",
            "LSTM (90,90,90) n_time_steps=12 epochs=100\n",
            "109/109 [==============================] - 15s 77ms/step - loss: 9257.7021\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 8096.4028\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 7334.9189\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 6620.4287\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 5982.3647\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 5443.7510\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 4972.9990\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 4562.9331\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 4200.2168\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 3887.8474\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 3604.8357\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 3358.2905\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 3135.1072\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 2937.3711\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 2762.3918\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 2607.4221\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 2465.4978\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 2331.1101\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 2218.1223\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 2107.8616\n",
            "109/109 [==============================] - 9s 83ms/step - loss: 2005.9270\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 1917.2098\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 1828.2168\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 1751.8517\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 1677.8831\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 1612.5787\n",
            "109/109 [==============================] - 9s 78ms/step - loss: 1537.0288\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 1487.8572\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 1423.8689\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 1388.6147\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 1338.1180\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 1284.7157\n",
            "109/109 [==============================] - 9s 83ms/step - loss: 1254.4391\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 1201.7227\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 1153.2151\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 1112.5490\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 1091.8489\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 1052.3818\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 1013.4841\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 992.8832\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 951.4157\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 929.4673\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 902.4986\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 902.7970\n",
            "109/109 [==============================] - 9s 79ms/step - loss: 857.6785\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 830.2349\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 807.1216\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 785.4474\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 773.6905\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 756.2640\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 745.4972\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 794.9645\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 729.3698\n",
            "109/109 [==============================] - 10s 92ms/step - loss: 693.1052\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 676.2198\n",
            "109/109 [==============================] - 9s 83ms/step - loss: 677.7173\n",
            "109/109 [==============================] - 9s 79ms/step - loss: 677.1384\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 670.1175\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 664.8422\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 644.7018\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 619.5643\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 648.8665\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 675.9828\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 604.0216\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 601.0382\n",
            "109/109 [==============================] - 10s 88ms/step - loss: 565.4860\n",
            "109/109 [==============================] - 10s 94ms/step - loss: 542.2780\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 524.2792\n",
            "109/109 [==============================] - 9s 86ms/step - loss: 529.7270\n",
            "109/109 [==============================] - 9s 83ms/step - loss: 503.8617\n",
            "109/109 [==============================] - 9s 85ms/step - loss: 491.1942\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 490.6614\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 476.7763\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 459.9750\n",
            "109/109 [==============================] - 9s 83ms/step - loss: 457.3691\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 453.9084\n",
            "109/109 [==============================] - 10s 89ms/step - loss: 445.5101\n",
            "109/109 [==============================] - 9s 83ms/step - loss: 439.7367\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 426.7847\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 422.6852\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 408.6759\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 409.8863\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 409.8305\n",
            "109/109 [==============================] - 9s 79ms/step - loss: 423.7856\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 396.9039\n",
            "109/109 [==============================] - 9s 80ms/step - loss: 389.2727\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 381.3531\n",
            "109/109 [==============================] - 9s 79ms/step - loss: 372.6125\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 371.8128\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 376.1826\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 359.2867\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 356.7124\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 333.1959\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 410.7210\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 375.9034\n",
            "109/109 [==============================] - 9s 84ms/step - loss: 396.4977\n",
            "109/109 [==============================] - 9s 82ms/step - loss: 346.6357\n",
            "109/109 [==============================] - 9s 81ms/step - loss: 299.3868\n",
            "109/109 [==============================] - 9s 83ms/step - loss: 297.2067\n",
            "109/109 [==============================] - 9s 83ms/step - loss: 302.3459\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['sunspot', 'LSTM (90,90,90) n_time_steps=12 epochs=100', 12, 799.7418853824596, 28.279708014448445, 19.643156433105467, 20.0923049748328, 9.46, 921.3835899829865]\n"
          ]
        }
      ],
      "source": [
        "#create file to results\n",
        "criar_arquivo_resultado()\n",
        "\n",
        "print('forecast for new sunspot number')\n",
        "num_epochs =100 # number of epochs for train\n",
        "batch_size = 24\n",
        "\n",
        "#n_time_steps, l1, l2, l3 = 1, 300, 300, 300  # 54.614556805389434, 37.75, 415.70987725257874\n",
        "n_time_steps, l1, l2, l3 = 1, 90, 90, 90    # 12.632897235160328, 6.94, 101.77265691757202]\n",
        "#n_time_steps, l1, l2, l3 = 1, 32, 64, 32    # 19.19441074455132, 10.93, 80.54896903038025]\n",
        "#previsao_LSTM('sunspot', dataset, n_time_steps, l1, l2, l3,num_epochs, batch_size)\n",
        "for n_time_steps in range(1,13): #predict with 1 to 12 past values of medition \n",
        "   previsao_LSTM('sunspot', dataset, n_time_steps, l1, l2, l3,num_epochs, batch_size)\n",
        "def random_modelo():\n",
        "  for n_time_steps in range(1,2): #predict with 1 to 12 past values of medition    \n",
        "    for l1 in range(4,50,6): # chose layer 1 nodes - min 2 and max 4\n",
        "        for l2 in range(2,50,6): # chose layer 2 nodes - min 4 and max 12\n",
        "            for l3 in range(2,50,6): # chose layer 3 nodes - min 6 and max 8\n",
        "                previsao_LSTM('sunspot', dataset, n_time_steps, l1, l2, l3,num_epochs, batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MEevt86uejTw"
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
