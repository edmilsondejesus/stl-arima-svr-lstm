{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install pandas\n",
        "#!pip install seaborn\n",
        "#!pip install openpyxl\n",
        "#!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "naOfrff2U2Xy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow import data\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import yfinance as yf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5iIpJyzkmkW",
        "outputId": "00380b11-657f-4c91-cbc3-1194bcb79e40"
      },
      "outputs": [],
      "source": [
        "path_name_results='../results/'\n",
        "file_result = 'Result_LSTM_IBM_stock_prices.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JPemdWEukqf-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "# Define the stock symbol (IBM)\n",
        "stock_symbol = \"IBM\"\n",
        "\n",
        "# Define the start and end dates for the historical data\n",
        "start_date = \"2017-01-01\"\n",
        "end_date = \"2023-09-21\"\n",
        "\n",
        "# Download historical data\n",
        "dataset = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "\n",
        "dataset=dataset.drop(columns=['High','Low', 'Close','Adj Close','Volume'])\n",
        "#dataset = pd.read_csv(f'{path_name}{file_data}', sep =';', encoding = 'latin1', decimal='.',usecols=[5])\n",
        "#dataset.columns = ['num_observations']\n",
        "#dataset.columns = ['year','month', 'date','total_sunspot_number','std_derivation','num_observations','def_prov_indicator']\n",
        "\n",
        "ds=pd.DataFrame()\n",
        "#ds['date']=dataset['Open'].index\n",
        "ds['num_observations']=dataset['Open'].values\n",
        "dataset=ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1690 entries, 0 to 1689\n",
            "Data columns (total 1 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   num_observations  1690 non-null   float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 13.3 KB\n"
          ]
        }
      ],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "num_observations    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checks if there are null variables\n",
        "dataset.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def salvar_resultado(nm_dataset, ds_best_param, n_time_steps, MSE, RMSE, MAE, MAPE, sMAPE, Duration):\n",
        "  #Script to write training cycle results\n",
        "  data = [nm_dataset, ds_best_param, n_time_steps, MSE, RMSE, MAE, MAPE, sMAPE, Duration]\n",
        "  fields = ['Dataset','Best Params','n_time_steps','MSE', 'RMSE', 'MAE', 'MAPE','sMAPE','Duration']\n",
        "  with open(f'{path_name_results}{file_result}', \"a\",newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file,delimiter=';')\n",
        "    writer.writerow(data)  \n",
        "  print(fields)\n",
        "  print(data)\n",
        "    \n",
        "#Script to create the results file\n",
        "def criar_arquivo_resultado():\n",
        "  fields = ['Dataset','Best Params','n_time_steps','MSE', 'RMSE', 'MAE','MAPE','sMAPE','Duration']\n",
        "  with open(f'{path_name_results}{file_result}', \"w\",newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file,delimiter=';')\n",
        "    writer.writerow(fields)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_matrix_dataset(dataset, n_time_steps=1):\n",
        " dX, dY = [], []\n",
        " for i in range(len(dataset)-n_time_steps-1):\n",
        "  a = dataset[i:(i+n_time_steps), 0]\n",
        "  dX.append(a)\n",
        "  dY.append(dataset[i + n_time_steps, 0])\n",
        " return np.array(dX), np.array(dY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "  \n",
        "def save_model(model,n_time_steps):\n",
        "  # serialize model to JSON\n",
        "  model_json = model.to_json()\n",
        "  with open(f'{path_name_results}model_{n_time_steps}.json', \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(f'{path_name_results}model_{n_time_steps}.h5')\n",
        "  print(\"Saved model to disk\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gera_resultado(y_test, predict,nm_dataset, resultado, n_time_steps, Duracao):\n",
        " #Mean Squared Error (Mean Squared Difference Between Estimated Values and Actual Values) - MSE\n",
        " MSE = mean_squared_error(y_test, predict)    \n",
        " #Square Root of Mean Error - RMSE\n",
        " RMSE = np.sqrt(mean_squared_error(y_test, predict))    \n",
        " #Mean Absolute Distance or Mean Absolute Error - MAE\n",
        " MAE= median_absolute_error(y_pred=predict, y_true = y_test) \n",
        "  \n",
        " #Calculate the MAPE (Mean Absolute Percentage Error)\n",
        " MAPE = ((np.mean(np.abs(y_test -predict) / (y_test)))) * 100   \n",
        "  \n",
        " sMAPE = round(\n",
        " \tnp.mean(\n",
        " \t\tnp.abs(predict - y_test) /\n",
        " \t\t((np.abs(predict) + np.abs(y_test)))\n",
        " \t)*100, 2\n",
        " ) \n",
        " salvar_resultado(nm_dataset, resultado, n_time_steps, MSE, RMSE, MAE, MAPE, sMAPE, Duracao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def previsao_LSTM(nm_dataset, dataset, n_time_steps, l1, l2, l3,num_epochs, batch_size ): \n",
        " #num_epochs = 100 # number of epochs for train\n",
        " #n_time_steps = 6\n",
        " #l1, l2, l3 = 32 , 64, 32  # 4.61 smape   \n",
        " #l1, l2, l3 = 64 , 64, 64  # 4.61 smape\n",
        " #nm_dataset ='sunspot'\n",
        " #Split dataset in treinam /  80% treinam  20% test\n",
        " dataset=np.array(dataset) \n",
        " nlinhas = int(len(dataset) * 0.80)\n",
        " test = dataset[nlinhas:len(dataset),:]  \n",
        " train = dataset[0:nlinhas,:] \n",
        " #  reshape into X=t and Y=t+1 ot n_time_steps by steps\n",
        " #n_time_steps = 5\n",
        " X_train, Y_train = create_matrix_dataset(train, n_time_steps)\n",
        " X_test, Y_test = create_matrix_dataset(test, n_time_steps) \n",
        " #X_train.shape , Y_train.shape , X_test.shape , Y_test.shape  \n",
        " #reshape input to be [samples, time steps, features]\n",
        " X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        " X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) \n",
        " # Stores the training execution start time\n",
        " Hora_Inicio = time.time()\n",
        "   \n",
        " resultado = \"LSTM (\" + str(l1) + \",\" + str(l2) + \",\" + str(l3) + \") n_time_steps=\" + str(n_time_steps) + str(\" epochs=\") + str(num_epochs)\n",
        " print(resultado)\n",
        " # create and fit the LSTM network\n",
        "\n",
        " steps_per_epoch = len(X_train) \n",
        " model = Sequential()\n",
        " model.add(LSTM(l1, batch_input_shape=(batch_size, n_time_steps, 1 ), stateful=True, return_sequences=True))\n",
        " model.add(LSTM(l2, batch_input_shape=(batch_size, n_time_steps, 1 ), stateful=True, return_sequences=True))\n",
        " model.add(LSTM(l3, batch_input_shape=(batch_size, n_time_steps, 1 ), stateful=True))\n",
        " model.add(Dense(1))\n",
        " model.compile(loss='mean_squared_error', optimizer='adam',run_eagerly=True)\n",
        " \n",
        " #equalize train data to be multiple of batch_size\n",
        " train_size = int(np.trunc(len(X_train) / batch_size))\n",
        " X_train = X_train[0:(train_size * batch_size),:]\n",
        " Y_train = Y_train[0:(train_size * batch_size)]\n",
        " \n",
        " #equalize test data to be multiple of batch_size\n",
        " test_size = int(np.trunc(len(X_test) / batch_size))\n",
        " X_test = X_test[0:(test_size * batch_size),:]\n",
        " Y_test = Y_test[0:(test_size * batch_size)]\n",
        " \n",
        " for i in range(num_epochs):\t\n",
        "   model.fit(X_train, Y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
        "   model.reset_states()\n",
        " \n",
        " #predict values to X_test with size multiple of batch_size\n",
        " predict = model.predict(X_test, batch_size=batch_size)   \n",
        " \n",
        " Y_test = [Y_test]\n",
        " predict = predict.reshape(-1,1)[0:batch_size]   \n",
        " Y_test=np.array(Y_test[0][0:batch_size]).reshape(batch_size,1) \n",
        " \n",
        " Hora_Fim = time.time()   \n",
        " #Calculate the duration of the training execution\n",
        " Duracao = Hora_Fim - Hora_Inicio   \n",
        " \n",
        " #calc metrics of error and save in file\n",
        " gera_resultado(Y_test, predict,nm_dataset, resultado, n_time_steps, Duracao)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forecast for IBM Stock prices\n",
            "LSTM (8,18,8) n_time_steps=1 epochs=100\n",
            "225/225 [==============================] - 6s 25ms/step - loss: 17240.0039\n",
            "225/225 [==============================] - 6s 25ms/step - loss: 16386.5117\n",
            "225/225 [==============================] - 5s 23ms/step - loss: 15823.9170\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 15304.4697\n",
            "225/225 [==============================] - 8s 38ms/step - loss: 14807.6250\n",
            "225/225 [==============================] - 11s 51ms/step - loss: 14327.5303\n",
            "225/225 [==============================] - 10s 41ms/step - loss: 13861.4521\n",
            "225/225 [==============================] - 12s 53ms/step - loss: 13407.7939\n",
            "225/225 [==============================] - 15s 67ms/step - loss: 12965.4434\n",
            "225/225 [==============================] - 19s 84ms/step - loss: 12533.6045\n",
            "225/225 [==============================] - 25s 110ms/step - loss: 12111.6904\n",
            "225/225 [==============================] - 26s 114ms/step - loss: 11699.2930\n",
            "225/225 [==============================] - 28s 123ms/step - loss: 11296.0977\n",
            "225/225 [==============================] - 22s 97ms/step - loss: 10901.8379\n",
            "225/225 [==============================] - 25s 110ms/step - loss: 10516.2852\n",
            "225/225 [==============================] - 38s 171ms/step - loss: 10139.2568\n",
            "225/225 [==============================] - 35s 154ms/step - loss: 9770.5752\n",
            "225/225 [==============================] - 26s 114ms/step - loss: 9410.0781\n",
            "225/225 [==============================] - 26s 116ms/step - loss: 9057.6611\n",
            "225/225 [==============================] - 26s 115ms/step - loss: 8713.1875\n",
            "225/225 [==============================] - 26s 114ms/step - loss: 8376.5781\n",
            "225/225 [==============================] - 28s 126ms/step - loss: 8047.7251\n",
            "225/225 [==============================] - 26s 114ms/step - loss: 7726.5518\n",
            "225/225 [==============================] - 23s 104ms/step - loss: 7413.0024\n",
            "225/225 [==============================] - 26s 118ms/step - loss: 7107.0000\n",
            "225/225 [==============================] - 25s 111ms/step - loss: 6808.4976\n",
            "225/225 [==============================] - 25s 113ms/step - loss: 6517.4424\n",
            "225/225 [==============================] - 30s 133ms/step - loss: 6233.7812\n",
            "225/225 [==============================] - 25s 109ms/step - loss: 5957.4814\n",
            "225/225 [==============================] - 24s 108ms/step - loss: 5688.4907\n",
            "225/225 [==============================] - 21s 95ms/step - loss: 5426.7783\n",
            "225/225 [==============================] - 21s 93ms/step - loss: 5172.3096\n",
            "225/225 [==============================] - 22s 97ms/step - loss: 4925.0371\n",
            "225/225 [==============================] - 22s 99ms/step - loss: 4684.9307\n",
            "225/225 [==============================] - 21s 95ms/step - loss: 4451.9531\n",
            "225/225 [==============================] - 27s 119ms/step - loss: 4226.0742\n",
            "225/225 [==============================] - 23s 100ms/step - loss: 4007.2480\n",
            "225/225 [==============================] - 24s 106ms/step - loss: 3795.4468\n",
            "225/225 [==============================] - 25s 110ms/step - loss: 3590.6270\n",
            "225/225 [==============================] - 26s 117ms/step - loss: 3392.7466\n",
            "225/225 [==============================] - 26s 117ms/step - loss: 3201.7695\n",
            "225/225 [==============================] - 24s 108ms/step - loss: 3017.6536\n",
            "225/225 [==============================] - 27s 120ms/step - loss: 2840.3506\n",
            "225/225 [==============================] - 27s 122ms/step - loss: 2669.8103\n",
            "225/225 [==============================] - 26s 114ms/step - loss: 2505.9932\n",
            "225/225 [==============================] - 22s 96ms/step - loss: 2348.8494\n",
            "225/225 [==============================] - 23s 102ms/step - loss: 2198.3157\n",
            "225/225 [==============================] - 24s 104ms/step - loss: 2054.3379\n",
            "225/225 [==============================] - 24s 107ms/step - loss: 1916.8546\n",
            "225/225 [==============================] - 25s 113ms/step - loss: 1785.7905\n",
            "225/225 [==============================] - 21s 95ms/step - loss: 1661.0845\n",
            "225/225 [==============================] - 20s 91ms/step - loss: 1542.6600\n",
            "225/225 [==============================] - 25s 110ms/step - loss: 1430.4366\n",
            "225/225 [==============================] - 20s 88ms/step - loss: 1324.3311\n",
            "225/225 [==============================] - 16s 70ms/step - loss: 1224.2527\n",
            "225/225 [==============================] - 20s 88ms/step - loss: 1130.0981\n",
            "225/225 [==============================] - 33s 147ms/step - loss: 1041.7701\n",
            "225/225 [==============================] - 23s 101ms/step - loss: 959.1497\n",
            "225/225 [==============================] - 19s 87ms/step - loss: 882.1165\n",
            "225/225 [==============================] - 20s 88ms/step - loss: 810.5399\n",
            "225/225 [==============================] - 18s 78ms/step - loss: 744.2844\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 683.1979\n",
            "225/225 [==============================] - 20s 88ms/step - loss: 627.1204\n",
            "225/225 [==============================] - 18s 80ms/step - loss: 575.8785\n",
            "225/225 [==============================] - 23s 102ms/step - loss: 529.2905\n",
            "225/225 [==============================] - 18s 79ms/step - loss: 487.1593\n",
            "225/225 [==============================] - 13s 60ms/step - loss: 449.2759\n",
            "225/225 [==============================] - 13s 60ms/step - loss: 415.4218\n",
            "225/225 [==============================] - 15s 65ms/step - loss: 385.3640\n",
            "225/225 [==============================] - 13s 59ms/step - loss: 358.8597\n",
            "225/225 [==============================] - 13s 59ms/step - loss: 335.6613\n",
            "225/225 [==============================] - 13s 59ms/step - loss: 315.5109\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 298.1430\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 283.2965\n",
            "225/225 [==============================] - 16s 72ms/step - loss: 270.7092\n",
            "225/225 [==============================] - 17s 74ms/step - loss: 260.1257\n",
            "225/225 [==============================] - 16s 73ms/step - loss: 251.2996\n",
            "225/225 [==============================] - 17s 74ms/step - loss: 243.9980\n",
            "225/225 [==============================] - 18s 79ms/step - loss: 238.0045\n",
            "225/225 [==============================] - 20s 88ms/step - loss: 233.1193\n",
            "225/225 [==============================] - 19s 83ms/step - loss: 229.1648\n",
            "225/225 [==============================] - 19s 83ms/step - loss: 225.9823\n",
            "225/225 [==============================] - 18s 79ms/step - loss: 223.4345\n",
            "225/225 [==============================] - 21s 94ms/step - loss: 221.4033\n",
            "225/225 [==============================] - 20s 89ms/step - loss: 219.7899\n",
            "225/225 [==============================] - 18s 81ms/step - loss: 218.5122\n",
            "225/225 [==============================] - 26s 115ms/step - loss: 217.5009\n",
            "225/225 [==============================] - 28s 124ms/step - loss: 216.7021\n",
            "225/225 [==============================] - 29s 129ms/step - loss: 216.0715\n",
            "225/225 [==============================] - 36s 161ms/step - loss: 215.5725\n",
            "225/225 [==============================] - 25s 112ms/step - loss: 215.1776\n",
            "225/225 [==============================] - 24s 107ms/step - loss: 214.8643\n",
            "225/225 [==============================] - 24s 109ms/step - loss: 214.6152\n",
            "225/225 [==============================] - 26s 117ms/step - loss: 214.4161\n",
            "225/225 [==============================] - 27s 122ms/step - loss: 214.2565\n",
            "225/225 [==============================] - 28s 124ms/step - loss: 214.1286\n",
            "225/225 [==============================] - 28s 127ms/step - loss: 214.0251\n",
            "225/225 [==============================] - 28s 124ms/step - loss: 213.9413\n",
            "225/225 [==============================] - 28s 123ms/step - loss: 213.8729\n",
            "225/225 [==============================] - 25s 111ms/step - loss: 213.8170\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=1 epochs=100', 1, 766.654952467458, 27.68853467533914, 26.8228759765625, 20.85456585327278, 11.65, 2176.0227727890015]\n",
            "LSTM (8,18,8) n_time_steps=2 epochs=100\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 16936.0742\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 16018.1670\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 15474.0342\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 14966.7041\n",
            "224/224 [==============================] - 31s 138ms/step - loss: 14480.1738\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 14009.5918\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 13552.4717\n",
            "224/224 [==============================] - 29s 132ms/step - loss: 13107.3076\n",
            "224/224 [==============================] - 30s 132ms/step - loss: 12673.1484\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 12249.2803\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 11835.1699\n",
            "224/224 [==============================] - 30s 135ms/step - loss: 11430.4248\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 11034.7012\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 10647.7363\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 10269.2900\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 9899.1826\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 9537.2383\n",
            "224/224 [==============================] - 29s 132ms/step - loss: 9183.3291\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 8837.3271\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 8499.1377\n",
            "224/224 [==============================] - 29s 132ms/step - loss: 8168.6533\n",
            "224/224 [==============================] - 30s 133ms/step - loss: 7845.8052\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 7530.5166\n",
            "224/224 [==============================] - 32s 141ms/step - loss: 7222.7334\n",
            "224/224 [==============================] - 30s 134ms/step - loss: 6922.3906\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 6629.4443\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 6343.8452\n",
            "224/224 [==============================] - 34s 151ms/step - loss: 6065.5479\n",
            "224/224 [==============================] - 31s 139ms/step - loss: 5794.5190\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 5530.7134\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 5274.0986\n",
            "224/224 [==============================] - 29s 128ms/step - loss: 5024.6304\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 4782.2871\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 4547.0259\n",
            "224/224 [==============================] - 30s 133ms/step - loss: 4318.8149\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 4097.6211\n",
            "224/224 [==============================] - 29s 128ms/step - loss: 3883.4019\n",
            "224/224 [==============================] - 30s 132ms/step - loss: 3676.1255\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 3475.7534\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 3282.2458\n",
            "224/224 [==============================] - 29s 128ms/step - loss: 3095.5591\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 2915.6528\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 2742.4875\n",
            "224/224 [==============================] - 30s 136ms/step - loss: 2576.0146\n",
            "224/224 [==============================] - 34s 151ms/step - loss: 2416.1714\n",
            "224/224 [==============================] - 30s 133ms/step - loss: 2262.9136\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 2116.1887\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 1975.9323\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 1842.0850\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 1714.5785\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 1593.3445\n",
            "224/224 [==============================] - 29s 128ms/step - loss: 1478.3018\n",
            "224/224 [==============================] - 29s 127ms/step - loss: 1369.3763\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 1266.4738\n",
            "224/224 [==============================] - 21s 96ms/step - loss: 1169.5068\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 1078.3701\n",
            "224/224 [==============================] - 22s 96ms/step - loss: 992.9604\n",
            "224/224 [==============================] - 21s 95ms/step - loss: 913.1622\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 838.8517\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 769.8971\n",
            "224/224 [==============================] - 21s 94ms/step - loss: 706.1569\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 647.4769\n",
            "224/224 [==============================] - 22s 96ms/step - loss: 593.6957\n",
            "224/224 [==============================] - 22s 97ms/step - loss: 544.6366\n",
            "224/224 [==============================] - 22s 96ms/step - loss: 500.1153\n",
            "224/224 [==============================] - 23s 101ms/step - loss: 459.9333\n",
            "224/224 [==============================] - 21s 95ms/step - loss: 423.8809\n",
            "224/224 [==============================] - 21s 95ms/step - loss: 391.7349\n",
            "224/224 [==============================] - 23s 101ms/step - loss: 363.2642\n",
            "224/224 [==============================] - 24s 106ms/step - loss: 338.2249\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 316.3671\n",
            "224/224 [==============================] - 22s 97ms/step - loss: 297.4342 0s - loss: 3\n",
            "224/224 [==============================] - 21s 96ms/step - loss: 281.1670\n",
            "224/224 [==============================] - 22s 97ms/step - loss: 267.3037\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 255.5879\n",
            "224/224 [==============================] - 23s 101ms/step - loss: 245.7693\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 237.6088\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 230.8805\n",
            "224/224 [==============================] - 22s 96ms/step - loss: 225.3754\n",
            "224/224 [==============================] - 22s 97ms/step - loss: 220.9032\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 217.2941\n",
            "224/224 [==============================] - 23s 101ms/step - loss: 214.3977\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 212.0859\n",
            "224/224 [==============================] - 23s 103ms/step - loss: 210.2479\n",
            "224/224 [==============================] - 22s 97ms/step - loss: 208.7921\n",
            "224/224 [==============================] - 23s 104ms/step - loss: 207.6412\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 206.7328\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 206.0167\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 205.4515\n",
            "224/224 [==============================] - 22s 100ms/step - loss: 205.0053\n",
            "224/224 [==============================] - 22s 100ms/step - loss: 204.6532\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 204.3733\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 204.1520\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 203.9746\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 203.8330\n",
            "224/224 [==============================] - 22s 100ms/step - loss: 203.7190\n",
            "224/224 [==============================] - 22s 100ms/step - loss: 203.6268\n",
            "224/224 [==============================] - 23s 103ms/step - loss: 203.5523\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 203.4913\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 203.4412\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=2 epochs=100', 2, 14.500916532318419, 3.8080068976195958, 2.923004150390625, 2.5041577433305475, 1.27, 2605.6125028133392]\n",
            "LSTM (8,18,8) n_time_steps=3 epochs=100\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 17111.8848\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 16288.1592\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 15745.3750\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 15235.8525\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 14746.3018\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 14272.2070\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 13811.2783\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 13362.1279\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 12923.8320\n",
            "224/224 [==============================] - 24s 108ms/step - loss: 12495.7412\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 12077.3486\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 11668.2549\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 11268.1553\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 10876.7812\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 10493.9160\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 10119.3633\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 9752.9707\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 9394.5986\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 9044.1289\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 8701.4678\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 8366.5176\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 8039.2061\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 7719.4614\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 7407.2290\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 7102.4429\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 6805.0581\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 6515.0356\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 6232.3271\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 5956.8989\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 5688.7051\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 5427.7168\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 5173.9009\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 4927.2168\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 4687.6338\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 4455.1221\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 4229.6416\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 4011.1628\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 3799.6477\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 3595.0576\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 3397.3589\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 3206.5071\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 3022.4595\n",
            "224/224 [==============================] - 26s 118ms/step - loss: 2845.1780\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 2674.6143\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 2510.7207\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 2353.4438\n",
            "224/224 [==============================] - 25s 114ms/step - loss: 2202.7346\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 2058.5371\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 1920.7872\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 1789.4166\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 1664.3646\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 1545.5549\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 1432.9047\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 1326.3362\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 1225.7610\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 1131.0812\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 1042.1924\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 958.9879\n",
            "224/224 [==============================] - 25s 114ms/step - loss: 881.3499\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 809.1509\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 742.2558\n",
            "224/224 [==============================] - 25s 114ms/step - loss: 680.5170\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 623.7748\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 571.8689\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 524.6196\n",
            "224/224 [==============================] - 25s 114ms/step - loss: 481.8333\n",
            "224/224 [==============================] - 27s 118ms/step - loss: 443.3058\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 408.8237\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 378.1612\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 351.0810\n",
            "224/224 [==============================] - 25s 114ms/step - loss: 327.3375\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 306.6759\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 288.8382\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 273.5644\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 260.5947\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 249.6731\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 240.5535\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 233.0014\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 226.7970\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 221.7377\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 217.6413\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 214.3447\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 211.7072\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 209.6063\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 207.9408\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 206.6232\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 205.5838\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 204.7642\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 204.1188\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 203.6096\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 203.2083\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 202.8908\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 202.6382\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 202.4380\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 202.2788\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 202.1504\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 202.0470\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 201.9639\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 201.8968\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 201.8411\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=3 epochs=100', 3, 8.410945259267464, 2.900162971156529, 2.1505508422851562, 1.8692699377435131, 0.94, 2550.0556733608246]\n",
            "LSTM (8,18,8) n_time_steps=4 epochs=100\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 17105.2344\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 16349.0107\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 15812.0625\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 15305.2754\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 14817.0830\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 14343.4980\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 13882.5225\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 13432.9766\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 12994.0713\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 12565.1758\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 12145.8818\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 11735.8008\n",
            "224/224 [==============================] - 23s 101ms/step - loss: 11334.6445\n",
            "224/224 [==============================] - 31s 136ms/step - loss: 10942.1650\n",
            "224/224 [==============================] - 110s 492ms/step - loss: 10558.1465\n",
            "224/224 [==============================] - 56s 249ms/step - loss: 10182.4199\n",
            "224/224 [==============================] - 58s 259ms/step - loss: 9814.8291\n",
            "224/224 [==============================] - 53s 236ms/step - loss: 9455.2480\n",
            "224/224 [==============================] - 54s 239ms/step - loss: 9103.5557\n",
            "224/224 [==============================] - 49s 218ms/step - loss: 8759.6582\n",
            "224/224 [==============================] - 39s 173ms/step - loss: 8423.4639\n",
            "224/224 [==============================] - 42s 189ms/step - loss: 8094.9062\n",
            "224/224 [==============================] - 43s 194ms/step - loss: 7773.9072\n",
            "224/224 [==============================] - 61s 273ms/step - loss: 7460.4160\n",
            "224/224 [==============================] - 54s 243ms/step - loss: 7154.3691\n",
            "224/224 [==============================] - 81s 361ms/step - loss: 6855.7314\n",
            "224/224 [==============================] - 47s 208ms/step - loss: 6564.4517\n",
            "224/224 [==============================] - 42s 186ms/step - loss: 6280.4883\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 6003.8018\n",
            "224/224 [==============================] - 32s 142ms/step - loss: 5734.3618\n",
            "224/224 [==============================] - 44s 197ms/step - loss: 5472.1196\n",
            "224/224 [==============================] - 50s 222ms/step - loss: 5217.0537\n",
            "224/224 [==============================] - 46s 207ms/step - loss: 4969.1245\n",
            "224/224 [==============================] - 44s 196ms/step - loss: 4728.3027\n",
            "224/224 [==============================] - 52s 231ms/step - loss: 4494.5552\n",
            "224/224 [==============================] - 44s 195ms/step - loss: 4267.8413\n",
            "224/224 [==============================] - 48s 212ms/step - loss: 4048.1335\n",
            "224/224 [==============================] - 50s 225ms/step - loss: 3835.3872\n",
            "224/224 [==============================] - 38s 168ms/step - loss: 3629.5774\n",
            "224/224 [==============================] - 37s 166ms/step - loss: 3430.6592\n",
            "224/224 [==============================] - 32s 142ms/step - loss: 3238.6038\n",
            "224/224 [==============================] - 23s 102ms/step - loss: 3053.3582\n",
            "224/224 [==============================] - 24s 106ms/step - loss: 2874.8811\n",
            "224/224 [==============================] - 22s 97ms/step - loss: 2703.1326\n",
            "224/224 [==============================] - 22s 98ms/step - loss: 2538.0630\n",
            "224/224 [==============================] - 24s 106ms/step - loss: 2379.6206\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 2227.7568\n",
            "224/224 [==============================] - 30s 134ms/step - loss: 2082.4153\n",
            "224/224 [==============================] - 26s 118ms/step - loss: 1943.5260\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 1811.0331\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 1684.8704\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 1564.9600\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 1451.2302\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 1343.5974\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1241.9719\n",
            "224/224 [==============================] - 32s 142ms/step - loss: 1146.2615\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 1056.3607\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 972.1676\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 893.5638\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 820.4200\n",
            "224/224 [==============================] - 24s 108ms/step - loss: 752.6033\n",
            "224/224 [==============================] - 37s 163ms/step - loss: 689.9743\n",
            "224/224 [==============================] - 42s 187ms/step - loss: 632.3738\n",
            "224/224 [==============================] - 51s 229ms/step - loss: 579.6378\n",
            "224/224 [==============================] - 54s 239ms/step - loss: 531.5889\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 488.0394\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 448.7885\n",
            "224/224 [==============================] - 25s 110ms/step - loss: 413.6234\n",
            "224/224 [==============================] - 24s 108ms/step - loss: 382.3167\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 354.6367\n",
            "224/224 [==============================] - 24s 107ms/step - loss: 330.3373\n",
            "224/224 [==============================] - 24s 107ms/step - loss: 309.1671\n",
            "224/224 [==============================] - 24s 109ms/step - loss: 290.8675\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 275.1781\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 261.8370\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 250.5886\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 241.1856\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 233.3893\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 226.9766\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 221.7427\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 217.5014\n",
            "224/224 [==============================] - 27s 118ms/step - loss: 214.0862\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 211.3528\n",
            "224/224 [==============================] - 30s 135ms/step - loss: 209.1750\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 207.4467\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 206.0802\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 205.0020\n",
            "224/224 [==============================] - 30s 135ms/step - loss: 204.1523\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 203.4833\n",
            "224/224 [==============================] - 27s 121ms/step - loss: 202.9566\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 202.5406\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 202.2121\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 201.9526\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 201.7460\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 201.5813\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 201.4495\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 201.3436\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 201.2576\n",
            "224/224 [==============================] - 27s 118ms/step - loss: 201.1886\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 201.1322\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=4 epochs=100', 4, 12.664593021501787, 3.558734750090513, 2.75, 2.2734227191008523, 1.15, 3352.0829622745514]\n",
            "LSTM (8,18,8) n_time_steps=5 epochs=100\n",
            "224/224 [==============================] - 29s 131ms/step - loss: 17203.1172\n",
            "224/224 [==============================] - 29s 127ms/step - loss: 16455.2773\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 15911.7734\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 15401.2168\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 14910.1582\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 14434.1445\n",
            "224/224 [==============================] - 34s 152ms/step - loss: 13971.0293\n",
            "224/224 [==============================] - 31s 138ms/step - loss: 13519.5107\n",
            "224/224 [==============================] - 29s 131ms/step - loss: 13078.7363\n",
            "224/224 [==============================] - 31s 136ms/step - loss: 12648.0859\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 12227.0830\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 11815.3477\n",
            "224/224 [==============================] - 29s 128ms/step - loss: 11412.5752\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 11018.5049\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 10632.9248\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 10255.6543\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 9886.5293\n",
            "224/224 [==============================] - 35s 156ms/step - loss: 9525.4229\n",
            "224/224 [==============================] - 32s 142ms/step - loss: 9172.2207\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 8826.8203\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 8489.1357\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 8159.0850\n",
            "224/224 [==============================] - 26s 118ms/step - loss: 7836.6069\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 7521.6348\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 7214.1235\n",
            "224/224 [==============================] - 26s 118ms/step - loss: 6914.0171\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 6621.2754\n",
            "224/224 [==============================] - 27s 121ms/step - loss: 6335.8525\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 6057.7153\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 5786.8296\n",
            "224/224 [==============================] - 26s 118ms/step - loss: 5523.1514\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 5266.6499\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 5017.2930\n",
            "224/224 [==============================] - 29s 127ms/step - loss: 4775.0425\n",
            "224/224 [==============================] - 30s 132ms/step - loss: 4539.8696\n",
            "224/224 [==============================] - 29s 127ms/step - loss: 4311.7446\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 4090.6294\n",
            "224/224 [==============================] - 29s 128ms/step - loss: 3876.4893\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 3669.2920\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 3468.9929\n",
            "224/224 [==============================] - 32s 144ms/step - loss: 3275.5581\n",
            "224/224 [==============================] - 63s 283ms/step - loss: 3088.9490\n",
            "224/224 [==============================] - 71s 319ms/step - loss: 2909.1157\n",
            "224/224 [==============================] - 52s 230ms/step - loss: 2736.0173\n",
            "224/224 [==============================] - 37s 164ms/step - loss: 2569.6167\n",
            "224/224 [==============================] - 37s 167ms/step - loss: 2409.8481\n",
            "224/224 [==============================] - 27s 118ms/step - loss: 2256.6680\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 2110.0178\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 1969.8373\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1836.0670\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 1708.6345\n",
            "224/224 [==============================] - 26s 114ms/step - loss: 1587.4749\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 1472.5127\n",
            "224/224 [==============================] - 34s 150ms/step - loss: 1363.6637\n",
            "224/224 [==============================] - 31s 140ms/step - loss: 1260.8412\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 1163.9540\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 1072.9026\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 987.5771\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 907.8646\n",
            "224/224 [==============================] - 29s 131ms/step - loss: 833.6423\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 764.7758\n",
            "224/224 [==============================] - 29s 127ms/step - loss: 701.1240\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 642.5359\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 588.8478\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 539.8856\n",
            "224/224 [==============================] - 27s 121ms/step - loss: 495.4594\n",
            "224/224 [==============================] - 32s 142ms/step - loss: 455.3744\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 419.4192\n",
            "224/224 [==============================] - 29s 131ms/step - loss: 387.3718\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 359.0003\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 334.0596\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 312.2991\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 293.4623\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 277.2878\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 263.5140\n",
            "224/224 [==============================] - 26s 118ms/step - loss: 251.8839\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 242.1477\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 234.0643\n",
            "224/224 [==============================] - 27s 121ms/step - loss: 227.4075\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 221.9678\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 217.5553\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 213.9995\n",
            "224/224 [==============================] - 26s 116ms/step - loss: 211.1511\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 208.8806\n",
            "224/224 [==============================] - 27s 118ms/step - loss: 207.0791\n",
            "224/224 [==============================] - 33s 145ms/step - loss: 205.6547\n",
            "224/224 [==============================] - 31s 136ms/step - loss: 204.5307\n",
            "224/224 [==============================] - 40s 180ms/step - loss: 203.6450\n",
            "224/224 [==============================] - 39s 172ms/step - loss: 202.9479\n",
            "224/224 [==============================] - 27s 121ms/step - loss: 202.3996\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 201.9673\n",
            "224/224 [==============================] - 34s 153ms/step - loss: 201.6257\n",
            "224/224 [==============================] - 30s 133ms/step - loss: 201.3558\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 201.1415\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 200.9707\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 200.8345\n",
            "224/224 [==============================] - 29s 131ms/step - loss: 200.7248\n",
            "224/224 [==============================] - 27s 121ms/step - loss: 200.6363\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 200.5651\n",
            "224/224 [==============================] - 27s 121ms/step - loss: 200.5070\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=5 epochs=100', 5, 19.575585933673818, 4.424430577336909, 3.6826095581054688, 2.730379630727542, 1.39, 2968.4114055633545]\n",
            "LSTM (8,18,8) n_time_steps=6 epochs=100\n",
            "224/224 [==============================] - 30s 133ms/step - loss: 17271.4473\n",
            "224/224 [==============================] - 34s 150ms/step - loss: 16548.5352\n",
            "224/224 [==============================] - 37s 167ms/step - loss: 16006.6328\n",
            "224/224 [==============================] - 49s 220ms/step - loss: 15495.9170\n",
            "224/224 [==============================] - 67s 300ms/step - loss: 15004.0801\n",
            "224/224 [==============================] - 41s 182ms/step - loss: 14527.0107\n",
            "224/224 [==============================] - 41s 182ms/step - loss: 14062.6641\n",
            "224/224 [==============================] - 35s 155ms/step - loss: 13609.8145\n",
            "224/224 [==============================] - 37s 167ms/step - loss: 13167.6426\n",
            "224/224 [==============================] - 35s 156ms/step - loss: 12735.5488\n",
            "224/224 [==============================] - 18s 82ms/step - loss: 12313.0645\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 11899.8271\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 11495.5312\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 11099.9248\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 10712.8018\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 10333.9727\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 9963.2988\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 9600.6357\n",
            "224/224 [==============================] - 23s 104ms/step - loss: 9245.8711\n",
            "224/224 [==============================] - 19s 86ms/step - loss: 8898.9102\n",
            "224/224 [==============================] - 19s 84ms/step - loss: 8559.6631\n",
            "224/224 [==============================] - 19s 84ms/step - loss: 8228.0557\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 7904.0200\n",
            "224/224 [==============================] - 21s 94ms/step - loss: 7587.4941\n",
            "224/224 [==============================] - 16s 74ms/step - loss: 7278.4292\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 6976.7759\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 6682.4883\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 6395.5259\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 6115.8530\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 5843.4253\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 5578.2148\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 5320.1919\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 5069.3164\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 4825.5518\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 4588.8735\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 4359.2476\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 4136.6382\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 3921.0066\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 3712.3225\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 3510.5505\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 3315.6418\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 3127.5723\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 2946.2925\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 2771.7642\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 2603.9285\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 2442.7424\n",
            "224/224 [==============================] - 19s 84ms/step - loss: 2288.1504\n",
            "224/224 [==============================] - 22s 99ms/step - loss: 2140.1060\n",
            "224/224 [==============================] - 19s 86ms/step - loss: 1998.5508\n",
            "224/224 [==============================] - 19s 85ms/step - loss: 1863.4155\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 1734.6349\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 1612.1436\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 1495.8647\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 1385.7173\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 1281.6191\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 1183.4719\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 1091.1812\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 1004.6417\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 923.7392\n",
            "224/224 [==============================] - 23s 102ms/step - loss: 848.3532\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 778.3539\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 713.6015\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 653.9471\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 599.2286\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 549.2751\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 503.8999\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 462.9086\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 426.0947\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 393.2366\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 364.1030\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 338.4537\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 316.0408\n",
            "224/224 [==============================] - 20s 92ms/step - loss: 296.6065\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 279.8905\n",
            "224/224 [==============================] - 20s 88ms/step - loss: 265.6318\n",
            "224/224 [==============================] - 19s 85ms/step - loss: 253.5719\n",
            "224/224 [==============================] - 19s 83ms/step - loss: 243.4579\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 235.0460\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 228.1084\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 222.4303\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 217.8185\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 214.0967\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 211.1129\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 208.7336\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 206.8428\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 205.3480\n",
            "224/224 [==============================] - 18s 82ms/step - loss: 204.1676\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 203.2381\n",
            "224/224 [==============================] - 18s 81ms/step - loss: 202.5066\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 201.9312\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 201.4773\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 201.1195\n",
            "224/224 [==============================] - 18s 82ms/step - loss: 200.8364\n",
            "224/224 [==============================] - 18s 81ms/step - loss: 200.6124\n",
            "224/224 [==============================] - 20s 91ms/step - loss: 200.4342\n",
            "224/224 [==============================] - 21s 91ms/step - loss: 200.2916\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 200.1776\n",
            "224/224 [==============================] - 20s 88ms/step - loss: 200.0859\n",
            "224/224 [==============================] - 20s 88ms/step - loss: 200.0112\n",
            "224/224 [==============================] - 20s 90ms/step - loss: 199.9507\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=6 epochs=100', 6, 27.138293712593924, 5.209442744919453, 5.3011016845703125, 3.3250986116241847, 1.7, 1965.6482424736023]\n",
            "LSTM (8,18,8) n_time_steps=7 epochs=100\n",
            "224/224 [==============================] - 18s 81ms/step - loss: 17117.4629\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 16319.8145\n",
            "224/224 [==============================] - 21s 94ms/step - loss: 15776.7842\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 15267.6846\n",
            "224/224 [==============================] - 20s 88ms/step - loss: 14778.3359\n",
            "224/224 [==============================] - 19s 84ms/step - loss: 14304.2275\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 13843.0967\n",
            "224/224 [==============================] - 18s 79ms/step - loss: 13393.6191\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 12954.9199\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 12526.3721\n",
            "224/224 [==============================] - 18s 81ms/step - loss: 12107.4795\n",
            "224/224 [==============================] - 18s 81ms/step - loss: 11697.8799\n",
            "224/224 [==============================] - 18s 82ms/step - loss: 11297.2383\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 10905.3076\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 10521.8701\n",
            "224/224 [==============================] - 19s 84ms/step - loss: 10146.7480\n",
            "224/224 [==============================] - 18s 82ms/step - loss: 9779.7715\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 9420.8096\n",
            "224/224 [==============================] - 18s 79ms/step - loss: 9069.7471\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 8726.4795\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 8390.9355\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 8063.0215\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 7742.6704\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 7429.8296\n",
            "224/224 [==============================] - 20s 89ms/step - loss: 7124.4346\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 6826.4390\n",
            "224/224 [==============================] - 18s 81ms/step - loss: 6535.8071\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 6252.4927\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 5976.4541\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 5707.6538\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 5446.0640\n",
            "224/224 [==============================] - 19s 83ms/step - loss: 5191.6401\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 4944.3564\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 4704.1733\n",
            "224/224 [==============================] - 19s 84ms/step - loss: 4471.0586\n",
            "224/224 [==============================] - 24s 108ms/step - loss: 4244.9775\n",
            "224/224 [==============================] - 21s 96ms/step - loss: 4025.8997\n",
            "224/224 [==============================] - 19s 85ms/step - loss: 3813.7849\n",
            "224/224 [==============================] - 21s 95ms/step - loss: 3608.6082\n",
            "224/224 [==============================] - 21s 92ms/step - loss: 3410.3198\n",
            "224/224 [==============================] - 23s 101ms/step - loss: 3218.8779\n",
            "224/224 [==============================] - 18s 82ms/step - loss: 3034.2507\n",
            "224/224 [==============================] - 21s 93ms/step - loss: 2856.3879\n",
            "224/224 [==============================] - 34s 153ms/step - loss: 2685.2520\n",
            "224/224 [==============================] - 24s 105ms/step - loss: 2520.7957\n",
            "224/224 [==============================] - 23s 102ms/step - loss: 2362.9570\n",
            "224/224 [==============================] - 26s 117ms/step - loss: 2211.6890\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 2066.9355\n",
            "224/224 [==============================] - 26s 115ms/step - loss: 1928.6357\n",
            "224/224 [==============================] - 24s 107ms/step - loss: 1796.7327\n",
            "224/224 [==============================] - 21s 93ms/step - loss: 1671.1490\n",
            "224/224 [==============================] - 20s 90ms/step - loss: 1551.8099\n",
            "224/224 [==============================] - 21s 95ms/step - loss: 1438.6475\n",
            "224/224 [==============================] - 23s 103ms/step - loss: 1331.5729\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 1230.5020\n",
            "224/224 [==============================] - 20s 91ms/step - loss: 1135.3339\n",
            "224/224 [==============================] - 29s 131ms/step - loss: 1045.9701\n",
            "224/224 [==============================] - 34s 152ms/step - loss: 962.3056\n",
            "224/224 [==============================] - 30s 132ms/step - loss: 884.2162\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 811.5809\n",
            "224/224 [==============================] - 25s 111ms/step - loss: 744.2610\n",
            "224/224 [==============================] - 41s 184ms/step - loss: 682.1151\n",
            "224/224 [==============================] - 42s 189ms/step - loss: 624.9869\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 572.7080\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 525.1024\n",
            "224/224 [==============================] - 54s 243ms/step - loss: 481.9774\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 443.1339\n",
            "224/224 [==============================] - 36s 163ms/step - loss: 408.3576\n",
            "224/224 [==============================] - 35s 157ms/step - loss: 377.4227\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 350.0917\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 326.1195\n",
            "224/224 [==============================] - 25s 112ms/step - loss: 305.2545\n",
            "224/224 [==============================] - 35s 156ms/step - loss: 287.2362\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 271.8035\n",
            "224/224 [==============================] - 27s 119ms/step - loss: 258.6971\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 247.6601\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 238.4449\n",
            "224/224 [==============================] - 34s 151ms/step - loss: 230.8148\n",
            "224/224 [==============================] - 28s 126ms/step - loss: 224.5464\n",
            "224/224 [==============================] - 33s 147ms/step - loss: 219.4374\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 215.3026\n",
            "224/224 [==============================] - 29s 128ms/step - loss: 211.9772\n",
            "224/224 [==============================] - 30s 133ms/step - loss: 209.3195\n",
            "224/224 [==============================] - 31s 139ms/step - loss: 207.2042\n",
            "224/224 [==============================] - 28s 125ms/step - loss: 205.5292\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 204.2052\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 203.1617\n",
            "224/224 [==============================] - 27s 120ms/step - loss: 202.3404\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 201.6939\n",
            "224/224 [==============================] - 25s 113ms/step - loss: 201.1857\n",
            "224/224 [==============================] - 40s 177ms/step - loss: 200.7849\n",
            "224/224 [==============================] - 41s 185ms/step - loss: 200.4686\n",
            "224/224 [==============================] - 36s 159ms/step - loss: 200.2183\n",
            "224/224 [==============================] - 40s 177ms/step - loss: 200.0195\n",
            "224/224 [==============================] - 30s 134ms/step - loss: 199.8617\n",
            "224/224 [==============================] - 27s 121ms/step - loss: 199.7346\n",
            "224/224 [==============================] - 29s 131ms/step - loss: 199.6330\n",
            "224/224 [==============================] - 29s 130ms/step - loss: 199.5509\n",
            "224/224 [==============================] - 31s 137ms/step - loss: 199.4839\n",
            "224/224 [==============================] - 28s 127ms/step - loss: 199.4296\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=7 epochs=100', 7, 37.13898500105521, 6.0941763185073015, 6.32861328125, 4.2001959365733095, 2.15, 2485.0052716732025]\n",
            "LSTM (8,18,8) n_time_steps=8 epochs=100\n",
            "223/223 [==============================] - 30s 136ms/step - loss: 17027.7676\n",
            "223/223 [==============================] - 34s 152ms/step - loss: 16234.0508\n",
            "223/223 [==============================] - 28s 127ms/step - loss: 15699.2822\n",
            "223/223 [==============================] - 31s 138ms/step - loss: 15196.0391\n",
            "223/223 [==============================] - 29s 129ms/step - loss: 14711.4619\n",
            "223/223 [==============================] - 29s 129ms/step - loss: 14241.4570\n",
            "223/223 [==============================] - 32s 143ms/step - loss: 13784.0254\n",
            "223/223 [==============================] - 33s 147ms/step - loss: 13337.9746\n",
            "223/223 [==============================] - 31s 139ms/step - loss: 12902.4756\n",
            "223/223 [==============================] - 29s 130ms/step - loss: 12476.9619\n",
            "223/223 [==============================] - 32s 142ms/step - loss: 12060.9697\n",
            "223/223 [==============================] - 33s 150ms/step - loss: 11654.1377\n",
            "223/223 [==============================] - 38s 169ms/step - loss: 11256.1592\n",
            "223/223 [==============================] - 37s 164ms/step - loss: 10866.7949\n",
            "223/223 [==============================] - 27s 122ms/step - loss: 10485.8340\n",
            "223/223 [==============================] - 30s 136ms/step - loss: 10113.1025\n",
            "223/223 [==============================] - 30s 135ms/step - loss: 9748.4307\n",
            "223/223 [==============================] - 28s 124ms/step - loss: 9391.7061\n",
            "223/223 [==============================] - 30s 134ms/step - loss: 9042.8096\n",
            "223/223 [==============================] - 28s 128ms/step - loss: 8701.6377\n",
            "223/223 [==============================] - 29s 131ms/step - loss: 8368.1045\n",
            "223/223 [==============================] - 30s 133ms/step - loss: 8042.1450\n",
            "223/223 [==============================] - 31s 140ms/step - loss: 7723.6821\n",
            "223/223 [==============================] - 30s 134ms/step - loss: 7412.6553\n",
            "223/223 [==============================] - 30s 133ms/step - loss: 7109.0171\n",
            "223/223 [==============================] - 33s 150ms/step - loss: 6812.7124\n",
            "223/223 [==============================] - 32s 143ms/step - loss: 6523.7012\n",
            "223/223 [==============================] - 31s 138ms/step - loss: 6241.9443\n",
            "223/223 [==============================] - 31s 141ms/step - loss: 5967.4004\n",
            "223/223 [==============================] - 31s 138ms/step - loss: 5700.0278\n",
            "223/223 [==============================] - 40s 178ms/step - loss: 5439.7949\n",
            "223/223 [==============================] - 33s 149ms/step - loss: 5186.6709\n",
            "223/223 [==============================] - 29s 129ms/step - loss: 4940.6221\n",
            "223/223 [==============================] - 29s 130ms/step - loss: 4701.6099\n",
            "223/223 [==============================] - 28s 125ms/step - loss: 4469.6094\n",
            "223/223 [==============================] - 32s 146ms/step - loss: 4244.5796\n",
            "223/223 [==============================] - 28s 125ms/step - loss: 4026.4875\n",
            "223/223 [==============================] - 29s 128ms/step - loss: 3815.3069\n",
            "223/223 [==============================] - 29s 131ms/step - loss: 3610.9866\n",
            "223/223 [==============================] - 43s 192ms/step - loss: 3413.4980\n",
            "223/223 [==============================] - 33s 150ms/step - loss: 3222.8022\n",
            "223/223 [==============================] - 33s 146ms/step - loss: 3038.8542\n",
            "223/223 [==============================] - 35s 158ms/step - loss: 2861.6213\n",
            "223/223 [==============================] - 37s 165ms/step - loss: 2691.0525\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 2527.0986\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 2369.7073\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 2218.8328\n",
            "223/223 [==============================] - 18s 81ms/step - loss: 2074.4175\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 1936.4054\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 1804.7242\n",
            "223/223 [==============================] - 21s 95ms/step - loss: 1679.3163\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 1560.1085\n",
            "223/223 [==============================] - 22s 99ms/step - loss: 1447.0216\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 1339.9735\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 1238.8832\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 1143.6571\n",
            "223/223 [==============================] - 21s 92ms/step - loss: 1054.1895\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 970.3750\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 892.1026\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 819.2441\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 751.6709\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 689.2398\n",
            "223/223 [==============================] - 18s 83ms/step - loss: 631.8000\n",
            "223/223 [==============================] - 19s 83ms/step - loss: 579.1840\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 531.2236\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 487.7291\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 448.5049\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 413.3390\n",
            "223/223 [==============================] - 20s 92ms/step - loss: 382.0113\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 354.2923\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 329.9388\n",
            "223/223 [==============================] - 19s 83ms/step - loss: 308.7019\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 290.3266\n",
            "223/223 [==============================] - 18s 82ms/step - loss: 274.5569\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 261.1335\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 249.8046\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 240.3228\n",
            "223/223 [==============================] - 18s 81ms/step - loss: 232.4522\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 225.9724\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 220.6783\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 216.3826\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 212.9206\n",
            "223/223 [==============================] - 19s 83ms/step - loss: 210.1469\n",
            "223/223 [==============================] - 18s 82ms/step - loss: 207.9364\n",
            "223/223 [==============================] - 18s 82ms/step - loss: 206.1804\n",
            "223/223 [==============================] - 22s 99ms/step - loss: 204.7914\n",
            "223/223 [==============================] - 32s 142ms/step - loss: 203.6945\n",
            "223/223 [==============================] - 25s 114ms/step - loss: 202.8302\n",
            "223/223 [==============================] - 19s 83ms/step - loss: 202.1493\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 201.6128\n",
            "223/223 [==============================] - 25s 114ms/step - loss: 201.1898\n",
            "223/223 [==============================] - 27s 119ms/step - loss: 200.8552\n",
            "223/223 [==============================] - 32s 144ms/step - loss: 200.5905\n",
            "223/223 [==============================] - 22s 96ms/step - loss: 200.3804\n",
            "223/223 [==============================] - 21s 96ms/step - loss: 200.2130\n",
            "223/223 [==============================] - 22s 100ms/step - loss: 200.0791\n",
            "223/223 [==============================] - 25s 113ms/step - loss: 199.9713\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 199.8848\n",
            "223/223 [==============================] - 18s 83ms/step - loss: 199.8141\n",
            "223/223 [==============================] - 19s 83ms/step - loss: 199.7569\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=8 epochs=100', 8, 54.825031784673534, 7.404392735712601, 7.0923919677734375, 5.133753984615194, 2.64, 2538.1642100811005]\n",
            "LSTM (8,18,8) n_time_steps=9 epochs=100\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 16943.9297\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 16076.1318\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 15541.4414\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 15039.5010\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 14556.7744\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 14088.8613\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 13633.6836\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 13189.9766\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 12756.8994\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 12333.8389\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 11920.3301\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 11516.0088\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 11120.5498\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 10733.7168\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 10355.2959\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 9985.1006\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 9622.9736\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 9268.7930\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 8922.4336\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 8583.8027\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 8252.8018\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 7929.3750\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 7613.4297\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 7304.9268\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 7003.7969\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 6710.0054\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 6423.4932\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 6144.2285\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 5872.1743\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 5607.2798\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 5349.5264\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 5098.8696\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 4855.2749\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 4618.7134\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 4389.1440\n",
            "223/223 [==============================] - 21s 94ms/step - loss: 4166.5430\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 3950.8696\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 3742.0864\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 3540.1572\n",
            "223/223 [==============================] - 19s 83ms/step - loss: 3345.0481\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 3156.7163\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 2975.1245\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 2800.2258\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 2631.9731\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 2470.3237\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 2315.2241\n",
            "223/223 [==============================] - 20s 87ms/step - loss: 2166.6208\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 2024.4563\n",
            "223/223 [==============================] - 21s 92ms/step - loss: 1888.6711\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 1759.1965\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 1635.9673\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 1518.9137\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 1407.9563\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 1303.0099\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 1203.9875\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 1110.7924\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 1023.3240\n",
            "223/223 [==============================] - 21s 95ms/step - loss: 941.4706\n",
            "223/223 [==============================] - 20s 92ms/step - loss: 865.1138\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 794.1288\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 728.3802\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 667.7223\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 611.9963\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 561.0406\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 514.6741\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 472.7088\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 434.9407\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 401.1551\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 371.1290\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 344.6285\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 321.4087\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 301.2156\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 283.7947\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 268.8886\n",
            "223/223 [==============================] - 21s 95ms/step - loss: 256.2406\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 245.5993\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 236.7209\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 229.3738\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 223.3419\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 218.4274\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 214.4514\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 211.2550\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 208.6993\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 206.6661\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 205.0535\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 203.7806\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 202.7758\n",
            "223/223 [==============================] - 21s 94ms/step - loss: 201.9848\n",
            "223/223 [==============================] - 19s 84ms/step - loss: 201.3617\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 200.8707\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 200.4833\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 200.1776\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 199.9350\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 199.7425\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 199.5887\n",
            "223/223 [==============================] - 19s 86ms/step - loss: 199.4658\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 199.3667\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 199.2867\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 199.2219\n",
            "223/223 [==============================] - 19s 85ms/step - loss: 199.1692\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=9 epochs=100', 9, 67.96995571507917, 8.244389347615696, 7.5018768310546875, 5.724060278078302, 2.95, 1945.955323457718]\n",
            "LSTM (8,18,8) n_time_steps=10 epochs=100\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 17247.6602\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 16531.9727\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 16016.4541\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 15551.3291\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 15107.1260\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 14677.3555\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 14259.2217\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 13851.1689\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 13452.2363\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 13061.7520\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 12679.2090\n",
            "223/223 [==============================] - 21s 95ms/step - loss: 12304.2441\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 11936.5557\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 11575.8887\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 11161.1084\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 10673.4121\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 10257.6357\n",
            "223/223 [==============================] - 21s 92ms/step - loss: 9867.3232\n",
            "223/223 [==============================] - 21s 92ms/step - loss: 9493.2344\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 9131.8213\n",
            "223/223 [==============================] - 21s 96ms/step - loss: 8781.2588\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 8440.4639\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 8108.7495\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 7785.6338\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 7470.7632\n",
            "223/223 [==============================] - 21s 96ms/step - loss: 7163.8901\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 6864.8115\n",
            "223/223 [==============================] - 20s 88ms/step - loss: 6573.3818\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 6289.4751\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 6012.9976\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 5743.8569\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 5481.9932\n",
            "223/223 [==============================] - 22s 99ms/step - loss: 5227.3374\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 4979.8413\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 4739.4482\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 4506.1196\n",
            "223/223 [==============================] - 23s 101ms/step - loss: 4279.8027\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 4060.4607\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 3848.0498\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 3642.5361\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 3443.8738\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 3252.0210\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 3066.9360\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 2888.5696\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 2716.8857\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 2551.8289\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 2393.3538\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 2241.4045\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 2095.9297\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 1956.8641\n",
            "223/223 [==============================] - 21s 94ms/step - loss: 1824.1490\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 1697.7131\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 1577.4915\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 1463.4091\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 1355.3828\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 1253.3274\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 1157.1492\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 1066.7518\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 982.0256\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 902.8588\n",
            "223/223 [==============================] - 20s 92ms/step - loss: 829.1329\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 760.7161\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 697.4651\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 639.2317\n",
            "223/223 [==============================] - 23s 101ms/step - loss: 585.8548\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 537.1605\n",
            "223/223 [==============================] - 20s 92ms/step - loss: 492.9655\n",
            "223/223 [==============================] - 20s 91ms/step - loss: 453.0760\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 417.2809\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 385.3634\n",
            "223/223 [==============================] - 22s 98ms/step - loss: 357.0933\n",
            "223/223 [==============================] - 22s 98ms/step - loss: 332.2298\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 310.5248\n",
            "223/223 [==============================] - 20s 90ms/step - loss: 291.7260\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 275.5736\n",
            "223/223 [==============================] - 22s 100ms/step - loss: 261.8111\n",
            "223/223 [==============================] - 21s 93ms/step - loss: 250.1831\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 240.4410\n",
            "223/223 [==============================] - 26s 118ms/step - loss: 232.3474\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 225.6779\n",
            "223/223 [==============================] - 21s 95ms/step - loss: 220.2239\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 215.7976\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 212.2288\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 209.3691\n",
            "223/223 [==============================] - 24s 107ms/step - loss: 207.0880\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 205.2784\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 203.8463\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 202.7167\n",
            "223/223 [==============================] - 24s 107ms/step - loss: 201.8264\n",
            "223/223 [==============================] - 26s 118ms/step - loss: 201.1259\n",
            "223/223 [==============================] - 19s 87ms/step - loss: 200.5737\n",
            "223/223 [==============================] - 20s 89ms/step - loss: 200.1394\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 199.7965\n",
            "223/223 [==============================] - 21s 94ms/step - loss: 199.5253\n",
            "223/223 [==============================] - 35s 158ms/step - loss: 199.3103\n",
            "223/223 [==============================] - 32s 145ms/step - loss: 199.1393\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 199.0022\n",
            "223/223 [==============================] - 22s 99ms/step - loss: 198.8927\n",
            "223/223 [==============================] - 29s 130ms/step - loss: 198.8048\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 198.7324\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=10 epochs=100', 10, 79.15926570227991, 8.897149302011286, 8.775779724121094, 6.211568471773699, 3.21, 2143.1487460136414]\n",
            "LSTM (8,18,8) n_time_steps=11 epochs=100\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 16994.1328\n",
            "223/223 [==============================] - 27s 122ms/step - loss: 16162.5293\n",
            "223/223 [==============================] - 25s 112ms/step - loss: 15623.7129\n",
            "223/223 [==============================] - 27s 122ms/step - loss: 15119.4062\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 14634.7158\n",
            "223/223 [==============================] - 26s 117ms/step - loss: 14165.0684\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 13708.2568\n",
            "223/223 [==============================] - 26s 117ms/step - loss: 13263.0059\n",
            "223/223 [==============================] - 24s 107ms/step - loss: 12828.4395\n",
            "223/223 [==============================] - 25s 112ms/step - loss: 12403.9473\n",
            "223/223 [==============================] - 29s 128ms/step - loss: 11989.0293\n",
            "223/223 [==============================] - 31s 140ms/step - loss: 11583.3135\n",
            "223/223 [==============================] - 25s 112ms/step - loss: 11186.5020\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 10798.3125\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 10418.5498\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 10047.0273\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 9683.5811\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 9328.0908\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 8980.4336\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 8640.5020\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 8308.2178\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 7983.4956\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 7666.2749\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 7356.4971\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 7054.0986\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 6759.0366\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 6471.2637\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 6190.7412\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 5917.4277\n",
            "223/223 [==============================] - 24s 106ms/step - loss: 5651.2896\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 5392.2886\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 5140.3896\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 4895.5615\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 4657.7637\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 4426.9707\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 4203.1489\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 3986.2639\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 3776.2766\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 3573.1501\n",
            "223/223 [==============================] - 24s 107ms/step - loss: 3376.8501\n",
            "223/223 [==============================] - 27s 123ms/step - loss: 3187.3394\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 3004.5649\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 2828.4983\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 2659.0908\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 2496.2874\n",
            "223/223 [==============================] - 24s 106ms/step - loss: 2340.0417\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 2190.3044\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 2047.0154\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 1910.1169\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 1779.5433\n",
            "223/223 [==============================] - 24s 106ms/step - loss: 1655.2291\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 1537.1061\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 1425.0913\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 1319.1044\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 1219.0587\n",
            "223/223 [==============================] - 24s 107ms/step - loss: 1124.8585\n",
            "223/223 [==============================] - 24s 105ms/step - loss: 1036.4062\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 953.5887\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 876.2912\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 804.3894\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 737.7523\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 676.2336\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 619.6766\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 567.9198\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 520.7866\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 478.0888\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 439.6274\n",
            "223/223 [==============================] - 26s 116ms/step - loss: 405.1888\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 374.5501\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 347.4791\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 323.7321\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 303.0598\n",
            "223/223 [==============================] - 24s 106ms/step - loss: 285.2049\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 269.9103\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 256.9178\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 245.9743\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 236.8353\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 229.2658\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 223.0468\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 217.9765\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 213.8718\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 210.5708\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 207.9313\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 205.8311\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 204.1665\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 202.8523\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 201.8158\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 201.0003\n",
            "223/223 [==============================] - 23s 104ms/step - loss: 200.3587\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 199.8539\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 199.4556\n",
            "223/223 [==============================] - 24s 105ms/step - loss: 199.1418\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 198.8933\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 198.6960\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 198.5391\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 198.4136\n",
            "223/223 [==============================] - 23s 105ms/step - loss: 198.3123\n",
            "223/223 [==============================] - 23s 103ms/step - loss: 198.2308\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 198.1651\n",
            "223/223 [==============================] - 23s 102ms/step - loss: 198.1110\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=11 epochs=100', 11, 79.79024947042733, 8.93253880318621, 8.743431091308594, 6.242770605950972, 3.22, 2367.2641677856445]\n",
            "LSTM (8,18,8) n_time_steps=12 epochs=100\n",
            "223/223 [==============================] - 26s 115ms/step - loss: 17071.5508\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 16249.0449\n",
            "223/223 [==============================] - 26s 116ms/step - loss: 15706.0479\n",
            "223/223 [==============================] - 24s 107ms/step - loss: 15198.8232\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 14712.0117\n",
            "223/223 [==============================] - 24s 107ms/step - loss: 14240.5352\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 13782.0850\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 13335.2842\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 12899.2393\n",
            "223/223 [==============================] - 25s 112ms/step - loss: 12473.3135\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 12057.0205\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 11649.9502\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 11251.8008\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 10862.3096\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 10481.2529\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 10108.4434\n",
            "223/223 [==============================] - 28s 126ms/step - loss: 9743.7344\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 9386.9766\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 9038.0547\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 8696.8770\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 8363.3418\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 8037.3799\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 7718.9199\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 7407.9043\n",
            "223/223 [==============================] - 25s 112ms/step - loss: 7104.2778\n",
            "223/223 [==============================] - 27s 122ms/step - loss: 6807.9883\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 6518.9956\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 6237.2588\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 5962.7334\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 5695.3828\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 5435.1772\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 5182.0791\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 4936.0522\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 4697.0674\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 4465.0913\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 4240.0894\n",
            "223/223 [==============================] - 26s 115ms/step - loss: 4022.0232\n",
            "223/223 [==============================] - 27s 119ms/step - loss: 3810.8650\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 3606.5728\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 3409.1086\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 3218.4343\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 3034.5122\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 2857.3000\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 2686.7559\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 2522.8333\n",
            "223/223 [==============================] - 25s 112ms/step - loss: 2365.4807\n",
            "223/223 [==============================] - 26s 118ms/step - loss: 2214.6436\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 2070.2576\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 1932.2778\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 1800.6376\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 1675.2697\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 1556.1024\n",
            "223/223 [==============================] - 26s 115ms/step - loss: 1443.0571\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 1336.0546\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 1235.0046\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 1139.8197\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 1050.3951\n",
            "223/223 [==============================] - 24s 107ms/step - loss: 966.6295\n",
            "223/223 [==============================] - 27s 120ms/step - loss: 888.4046\n",
            "223/223 [==============================] - 29s 128ms/step - loss: 815.5967\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 748.0763\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 685.6964\n",
            "223/223 [==============================] - 26s 115ms/step - loss: 628.3091\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 575.7511\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 527.8483\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 484.4149\n",
            "223/223 [==============================] - 25s 113ms/step - loss: 445.2509\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 410.1489\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 378.8857\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 351.2303\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 326.9412\n",
            "223/223 [==============================] - 25s 113ms/step - loss: 305.7677\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 287.4561\n",
            "223/223 [==============================] - 26s 116ms/step - loss: 271.7490\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 258.3880\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 247.1194\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 237.6942\n",
            "223/223 [==============================] - 25s 113ms/step - loss: 229.8786\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 223.4489\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 218.2011\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 213.9483\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 210.5241\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 207.7852\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 205.6039\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 203.8746\n",
            "223/223 [==============================] - 24s 108ms/step - loss: 202.5083\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 201.4311\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 200.5837\n",
            "223/223 [==============================] - 27s 121ms/step - loss: 199.9165\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 199.3920\n",
            "223/223 [==============================] - 24s 109ms/step - loss: 198.9787\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 198.6526\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 198.3949\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 198.1905\n",
            "223/223 [==============================] - 28s 127ms/step - loss: 198.0279\n",
            "223/223 [==============================] - 26s 115ms/step - loss: 197.8980\n",
            "223/223 [==============================] - 25s 110ms/step - loss: 197.7935\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 197.7095\n",
            "223/223 [==============================] - 25s 111ms/step - loss: 197.6415\n",
            "223/223 [==============================] - 24s 110ms/step - loss: 197.5861\n",
            "['Dataset', 'Best Params', 'n_time_steps', 'MSE', 'RMSE', 'MAE', 'MAPE', 'sMAPE', 'Duration']\n",
            "['IBM', 'LSTM (8,18,8) n_time_steps=12 epochs=100', 12, 76.80866593553219, 8.764055336174698, 8.768539428710938, 6.071079933475745, 3.13, 2486.28653049469]\n"
          ]
        }
      ],
      "source": [
        "        \n",
        "#create file to results\n",
        "criar_arquivo_resultado()\n",
        "\n",
        "print('forecast for IBM Stock prices')\n",
        "num_epochs = 100 # number of epochs for train\n",
        "batch_size = 6\n",
        "\n",
        "l1, l2, l3 = 8, 18, 8    # 90.6444863489101, 86.26, 72.7875263690948\n",
        "\n",
        "for n_time_steps in range(1,13): #predict with 1 to 12 past values of medition \n",
        "    previsao_LSTM('IBM', dataset, n_time_steps, l1, l2, l3,num_epochs, batch_size)\n",
        "\n",
        "#\n",
        "def random_model():\n",
        "  for n_time_steps in range(1,2): #predict with 1 to 12 past values of medition    \n",
        "    for l1 in range(8,101,10): # chose layer 1 nodes - min 2 and max 4\n",
        "        for l2 in range(8,101,10): # chose layer 2 nodes - min 4 and max 12\n",
        "            for l3 in range(8,101,10): # chose layer 3 nodes - min 6 and max 8\n",
        "                previsao_LSTM('IBM', dataset, n_time_steps, l1, l2, l3,num_epochs, batch_size)            "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MEevt86uejTw"
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
